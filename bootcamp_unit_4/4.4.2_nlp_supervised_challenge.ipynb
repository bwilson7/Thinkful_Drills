{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /Users/brien/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1a244fb048>"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "def amount_punc(sentence):\n",
    "    #selecting only the punctuation in the sentence\n",
    "    all_punc = [token\n",
    "                for token in sentence\n",
    "                if token.is_punct]\n",
    "    #returning the amount of punctuation\n",
    "    return len(all_punc)\n",
    "\n",
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_pos(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allpos = [token.pos_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allpos).most_common(100)]\n",
    "\n",
    "def pos_features(sentences, common_pos):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_pos)\n",
    "    df['text_sentence'] = sentences\n",
    "    df.loc[:, common_pos] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        pos = [token.pos_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.pos_ in common_pos\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in pos:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice[:int(len(alice)/10)])\n",
    "persuasion = text_cleaner(persuasion[:int(len(persuasion)/10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                                      (Oh, dear, !)  Carroll"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suffering</th>\n",
       "      <th>shrubbery</th>\n",
       "      <th>depend</th>\n",
       "      <th>practice</th>\n",
       "      <th>raise</th>\n",
       "      <th>alas</th>\n",
       "      <th>tenant</th>\n",
       "      <th>dispose</th>\n",
       "      <th>fortitude</th>\n",
       "      <th>accurately</th>\n",
       "      <th>...</th>\n",
       "      <th>society</th>\n",
       "      <th>Rabbit</th>\n",
       "      <th>tempt</th>\n",
       "      <th>garden</th>\n",
       "      <th>effect</th>\n",
       "      <th>maintenance</th>\n",
       "      <th>tiny</th>\n",
       "      <th>grove</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1614 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  suffering shrubbery depend practice raise alas tenant dispose fortitude  \\\n",
       "0         0         0      0        0     0    0      0       0         0   \n",
       "1         0         0      0        0     0    0      0       0         0   \n",
       "2         0         0      0        0     0    0      0       0         0   \n",
       "3         0         0      0        0     0    0      0       0         0   \n",
       "4         0         0      0        0     0    0      0       0         0   \n",
       "\n",
       "  accurately     ...     society Rabbit tempt garden effect maintenance tiny  \\\n",
       "0          0     ...           0      0     0      0      0           0    0   \n",
       "1          0     ...           0      1     0      0      0           0    0   \n",
       "2          0     ...           0      1     0      0      0           0    0   \n",
       "3          0     ...           0      0     0      0      0           0    0   \n",
       "4          0     ...           0      0     0      0      0           0    0   \n",
       "\n",
       "  grove                                      text_sentence text_source  \n",
       "0     0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1     0  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2     0  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3     0                                      (Oh, dear, !)     Carroll  \n",
       "4     0                                      (Oh, dear, !)     Carroll  \n",
       "\n",
       "[5 rows x 1614 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.981203007518797\n",
      "\n",
      "Test set score: 0.8314606741573034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266, 1612) (266,)\n",
      "Training set score: 0.9774436090225563\n",
      "\n",
      "Test set score: 0.8707865168539326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9774436090225563\n",
      "\n",
      "Test set score: 0.8258426966292135\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "# Clean the Emma data.\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
    "emma = text_cleaner(emma[:int(len(emma)/60)])\n",
    "print(emma[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse our cleaned data.\n",
    "emma_doc = nlp(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seem\n",
      "unite\n",
      "have\n",
      "live\n",
      "vex\n",
      "be\n",
      "have\n",
      "be\n",
      "have\n",
      "die\n",
      "have\n",
      "have\n",
      "be\n",
      "supply\n",
      "have\n",
      "fall\n",
      "have\n",
      "be\n",
      "be\n",
      "have\n",
      "cease\n",
      "hold\n",
      "have\n",
      "allow\n",
      "impose\n",
      "be\n",
      "pass\n",
      "have\n",
      "be\n",
      "live\n",
      "attach\n",
      "do\n",
      "like\n",
      "esteem\n",
      "direct\n",
      "be\n",
      "have\n",
      "think\n",
      "be\n",
      "threaten\n",
      "be\n",
      "do\n",
      "come\n",
      "marry\n",
      "be\n",
      "bring\n",
      "be\n",
      "sit\n",
      "go\n",
      "be\n",
      "leave\n",
      "cheer\n",
      "compose\n",
      "sleep\n",
      "have\n",
      "sit\n",
      "think\n",
      "have\n",
      "lose\n",
      "have\n",
      "be\n",
      "be\n",
      "consider\n",
      "deny\n",
      "have\n",
      "wish\n",
      "promote\n",
      "be\n",
      "would\n",
      "be\n",
      "feel\n",
      "recall\n",
      "have\n",
      "teach\n",
      "have\n",
      "play\n",
      "have\n",
      "devote\n",
      "attach\n",
      "amuse\n",
      "nurse\n",
      "be\n",
      "owe\n",
      "have\n",
      "follow\n",
      "be\n",
      "leave\n",
      "be\n",
      "have\n",
      "be\n",
      "possess\n",
      "inform\n",
      "know\n",
      "could\n",
      "speak\n",
      "arise\n",
      "have\n",
      "could\n",
      "find\n",
      "be\n",
      "bear\n",
      "be\n",
      "be\n",
      "go\n",
      "be\n",
      "must\n",
      "be\n",
      "be\n",
      "suffer\n",
      "love\n",
      "be\n",
      "could\n",
      "meet\n",
      "have\n",
      "marry\n",
      "be\n",
      "increase\n",
      "have\n",
      "be\n",
      "be\n",
      "could\n",
      "have\n",
      "recommend\n",
      "remove\n",
      "be\n",
      "settle\n",
      "be\n",
      "must\n",
      "be\n",
      "struggle\n",
      "bring\n",
      "fill\n",
      "give\n",
      "amount\n",
      "do\n",
      "belong\n",
      "afford\n",
      "be\n",
      "look\n",
      "have\n",
      "be\n",
      "could\n",
      "be\n",
      "accept\n",
      "be\n",
      "could\n",
      "sigh\n",
      "wish\n",
      "awake\n",
      "make\n",
      "be\n",
      "require\n",
      "be\n",
      "be\n",
      "use\n",
      "hate\n",
      "part\n",
      "hate\n",
      "be\n",
      "be\n",
      "reconcile\n",
      "could\n",
      "speak\n",
      "have\n",
      "be\n",
      "be\n",
      "oblige\n",
      "part\n",
      "be\n",
      "suppose\n",
      "could\n",
      "feel\n",
      "be\n",
      "think\n",
      "have\n",
      "do\n",
      "would\n",
      "have\n",
      "be\n",
      "have\n",
      "spend\n",
      "smile\n",
      "chat\n",
      "could\n",
      "keep\n",
      "come\n",
      "be\n",
      "say\n",
      "have\n",
      "say\n",
      "wish\n",
      "be\n",
      "be\n",
      "think\n",
      "agree\n",
      "papa\n",
      "know\n",
      "be\n",
      "deserve\n",
      "would\n",
      "have\n",
      "have\n",
      "live\n",
      "bear\n",
      "may\n",
      "have\n",
      "be\n",
      "be\n",
      "have\n",
      "shall\n",
      "be\n",
      "go\n",
      "see\n",
      "come\n",
      "see\n",
      "shall\n",
      "be\n",
      "meet\n",
      "must\n",
      "begin\n",
      "must\n",
      "go\n",
      "pay\n",
      "be\n",
      "get\n",
      "be\n",
      "could\n",
      "walk\n",
      "think\n",
      "must\n",
      "go\n",
      "be\n",
      "will\n",
      "like\n",
      "put\n",
      "be\n",
      "be\n",
      "be\n",
      "pay\n",
      "be\n",
      "be\n",
      "put\n",
      "know\n",
      "have\n",
      "settle\n",
      "talk\n",
      "may\n",
      "be\n",
      "will\n",
      "like\n",
      "go\n",
      "be\n",
      "housemaid\n",
      "doubt\n",
      "will\n",
      "take\n",
      "be\n",
      "do\n",
      "papa\n",
      "get\n",
      "think\n",
      "mention\n",
      "be\n",
      "be\n",
      "do\n",
      "think\n",
      "be\n",
      "would\n",
      "have\n",
      "have\n",
      "think\n",
      "slight\n",
      "be\n",
      "will\n",
      "make\n",
      "be\n",
      "speak\n",
      "have\n",
      "see\n",
      "curtsey\n",
      "ask\n",
      "do\n",
      "have\n",
      "have\n",
      "do\n",
      "observe\n",
      "turn\n",
      "bang\n",
      "be\n",
      "will\n",
      "be\n",
      "will\n",
      "be\n",
      "have\n",
      "be\n",
      "use\n",
      "see\n",
      "go\n",
      "see\n",
      "know\n",
      "will\n",
      "be\n",
      "hear\n",
      "will\n",
      "be\n",
      "tell\n",
      "be\n",
      "spare\n",
      "maintain\n",
      "hope\n",
      "get\n",
      "be\n",
      "attack\n",
      "be\n",
      "place\n",
      "walk\n",
      "make\n",
      "be\n",
      "connect\n",
      "live\n",
      "be\n",
      "come\n",
      "have\n",
      "return\n",
      "walk\n",
      "say\n",
      "be\n",
      "be\n",
      "animate\n",
      "have\n",
      "do\n",
      "be\n",
      "answer\n",
      "be\n",
      "observe\n",
      "be\n",
      "come\n",
      "call\n",
      "be\n",
      "must\n",
      "have\n",
      "have\n",
      "be\n",
      "must\n",
      "draw\n",
      "must\n",
      "have\n",
      "find\n",
      "wish\n",
      "may\n",
      "catch\n",
      "look\n",
      "be\n",
      "have\n",
      "have\n",
      "rain\n",
      "be\n",
      "want\n",
      "put\n",
      "have\n",
      "wish\n",
      "be\n",
      "must\n",
      "be\n",
      "feel\n",
      "have\n",
      "be\n",
      "hope\n",
      "go\n",
      "do\n",
      "behave\n",
      "cry\n",
      "tis\n",
      "please\n",
      "say\n",
      "have\n",
      "come\n",
      "must\n",
      "be\n",
      "have\n",
      "please\n",
      "be\n",
      "say\n",
      "be\n",
      "have\n",
      "know\n",
      "would\n",
      "say\n",
      "be\n",
      "believe\n",
      "be\n",
      "say\n",
      "be\n",
      "be\n",
      "do\n",
      "think\n",
      "could\n",
      "mean\n",
      "suppose\n",
      "mean\n",
      "mean\n",
      "love\n",
      "find\n",
      "know\n",
      "be\n",
      "say\n",
      "like\n",
      "be\n",
      "could\n",
      "see\n",
      "tell\n",
      "be\n",
      "know\n",
      "would\n",
      "be\n",
      "would\n",
      "have\n",
      "suspect\n",
      "be\n",
      "think\n",
      "know\n",
      "flatter\n",
      "say\n",
      "mean\n",
      "have\n",
      "be\n",
      "use\n",
      "have\n",
      "please\n",
      "will\n",
      "have\n",
      "be\n",
      "must\n",
      "be\n",
      "say\n",
      "let\n",
      "pass\n",
      "want\n",
      "hear\n",
      "shall\n",
      "be\n",
      "tell\n",
      "behave\n",
      "be\n",
      "be\n",
      "see\n",
      "feel\n",
      "be\n",
      "go\n",
      "be\n",
      "be\n",
      "bear\n",
      "say\n",
      "be\n",
      "lose\n",
      "be\n",
      "will\n",
      "miss\n",
      "think\n",
      "turn\n",
      "divide\n",
      "be\n",
      "should\n",
      "miss\n",
      "say\n",
      "should\n",
      "like\n",
      "do\n",
      "could\n",
      "suppose\n",
      "know\n",
      "be\n",
      "know\n",
      "must\n",
      "be\n",
      "be\n",
      "settle\n",
      "be\n",
      "allow\n",
      "feel\n",
      "must\n",
      "be\n",
      "have\n",
      "have\n",
      "forget\n",
      "say\n",
      "make\n",
      "make\n",
      "know\n",
      "have\n",
      "take\n",
      "be\n",
      "prove\n",
      "say\n",
      "would\n",
      "marry\n",
      "may\n",
      "comfort\n",
      "shake\n",
      "reply\n",
      "wish\n",
      "would\n",
      "make\n",
      "say\n",
      "come\n",
      "pass\n",
      "pray\n",
      "do\n",
      "make\n",
      "promise\n",
      "make\n",
      "must\n",
      "be\n",
      "know\n",
      "say\n",
      "would\n",
      "marry\n",
      "have\n",
      "be\n",
      "seem\n",
      "occupy\n",
      "go\n",
      "need\n",
      "spend\n",
      "do\n",
      "like\n",
      "would\n",
      "marry\n",
      "talk\n",
      "let\n",
      "be\n",
      "talk\n",
      "believe\n",
      "meet\n",
      "begin\n",
      "drizzle\n",
      "dart\n",
      "borrow\n",
      "make\n"
     ]
    }
   ],
   "source": [
    "#for chunk in emma_doc.noun_chunks:\n",
    "#    print(chunk.text)\n",
    "for token in emma_doc:\n",
    "    if token.pos_ =='VERB':\n",
    "        print(token.lemma_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Build a new Bag of Words data frame for Emma word counts.\n",
    "# We'll use the same common words from Alice and Persuasion.\n",
    "emma_sentences = pd.DataFrame(emma_sents)\n",
    "emma_bow = bow_features(emma_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['suffering', 'shrubbery', 'depend', 'practice', 'raise', 'alas',\n",
       "       'tenant', 'dispose', 'fortitude', 'accurately',\n",
       "       ...\n",
       "       'society', 'Rabbit', 'tempt', 'garden', 'effect', 'maintenance', 'tiny',\n",
       "       'grove', 'text_sentence', 'text_source'],\n",
       "      dtype='object', length=1614)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma_bow.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.7073170731707317\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>155</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>57</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Austen      155       15\n",
       "Carroll      57       19"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can model it!\n",
    "# Let's use logistic regression again.\n",
    "\n",
    "# Combine the Emma sentence data with the Alice data from the test set.\n",
    "X_Emma_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    emma_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\n",
    "lr_Emma_predicted = lr.predict(X_Emma_test)\n",
    "pd.crosstab(y_Emma_test, lr_Emma_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge 0:\n",
    "See if can improve test set model accuracy to over 90% by making new features, using different modeling techniques, anything else that I can think of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_bow.loc[:, 'is_emma'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_books = pd.concat([emma_bow, word_counts])\n",
    "all_books.loc[all_books.is_emma.isna(), 'is_emma'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    444\n",
       "1.0    170\n",
       "Name: is_emma, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_books.is_emma.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books['punctuation_count'] = pd.Series(amount_punc(i) for i in all_books.text_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the bags.\n",
    "alicepunc = bag_o_punc(alice_doc)\n",
    "persuasionpunc = bag_o_punc(persuasion_doc)\n",
    "emmapunc = bag_o_punc(emma_doc)\n",
    "\n",
    "# Combine bags to create a set of unique punctuation\n",
    "common_punc = set(alicepunc + persuasionpunc + emmapunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      " ['NOUN', 'VERB', 'ADJ', 'PROPN', 'ADV', 'NUM', 'INTJ', 'DET', 'ADP', 'PART', 'PUNCT']\n",
      "persuasion\n",
      " ['NOUN', 'VERB', 'PROPN', 'ADJ', 'ADV', 'NUM', 'ADP', 'PUNCT', 'INTJ', 'DET', 'PART']\n",
      "alice\n",
      " ['NOUN', 'VERB', 'ADJ', 'PROPN', 'ADV', 'INTJ', 'ADP', 'AUX', 'NUM']\n"
     ]
    }
   ],
   "source": [
    "emmapos = bag_of_pos(emma_doc)\n",
    "persuasionpos = bag_of_pos(persuasion_doc)\n",
    "alicepos = bag_of_pos(alice_doc)\n",
    "\n",
    "print('emma\\n', emmapos)\n",
    "print('persuasion\\n', persuasionpos)\n",
    "print('alice\\n', alicepos)\n",
    "\n",
    "common_pos = set(emmapos + alicepos + persuasionpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n",
      "Processing row 450\n",
      "Processing row 500\n",
      "Processing row 550\n",
      "Processing row 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "all_books = all_books.reset_index(drop=True)\n",
    "pos = pos_features(all_books['text_sentence'], common_pos)\n",
    "all_books = pd.concat([pos, all_books])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Austen     485\n",
       "Carroll    129\n",
       "Name: text_source, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_books['text_source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5618729096989966\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>166</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>127</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0        Austen  Carroll\n",
       "text_source                 \n",
       "Austen          166        4\n",
       "Carroll         127        2"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_emma_test = all_books.loc[(all_books['text_source'] == 'Carroll') | \n",
    "                            (all_books['is_emma'] == 1.0)\n",
    "                           ]\n",
    "\n",
    "Y_emma_test = X_emma_test.loc[:, 'text_source']\n",
    "\n",
    "X_emma_test = X_emma_test.drop(['text_sentence', 'text_source', 'is_emma'], axis=1)\n",
    "\n",
    "print(lr.score(X_emma_test, Y_emma_test))\n",
    "lr_emma_pred = lr.predict(X_emma_test)\n",
    "pd.crosstab(Y_emma_test, lr_emma_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in features for the amount of punctuation in each sentence and a count of the different parts of speech in each sentence shot our Austen recall through the roof to 98%. However, our precision is pretty bad at 57%. The model didn't necessarily predict Austen correctly, because it predicted everything the be Austen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge 1:\n",
    "Find out if new model is good at identifying Alice in Wonderland vs. any other work. \n",
    "\n",
    "I'm going with Alice in Wonderland vs. Moby Dick\n",
    "'melville-moby_dick.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buster Bear yawned as he lay on his comfortable bed of leaves and watched the first early morning su\n"
     ]
    }
   ],
   "source": [
    "burg = gutenberg.raw('burgess-busterbrown.txt')\n",
    "pattern = r'\\b[A-Z]+\\b'\n",
    "burg = re.sub(pattern, '', burg)\n",
    "burg = text_cleaner(burg[:int(len(burg)/8)])\n",
    "print(burg[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "burg_doc = nlp(burg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "burg_sents = [[sent, 'Burgess'] for sent in burg_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n"
     ]
    }
   ],
   "source": [
    "burg_sentences = pd.DataFrame(burg_sents)\n",
    "burg_bow = bow_features(burg_sentences, common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_books = pd.concat([all_books, burg_bow])\n",
    "all_books = all_books.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books['punctuation_count'] = pd.Series(amount_punc(i) for i in all_books.text_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "burgess\n",
      " ['VERB', 'NOUN', 'PROPN', 'ADJ', 'ADV', 'ADP', 'INTJ']\n"
     ]
    }
   ],
   "source": [
    "burgpos = bag_of_pos(burg_doc)\n",
    "print('burgess\\n', burgpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n",
      "Processing row 450\n",
      "Processing row 500\n",
      "Processing row 550\n",
      "Processing row 600\n",
      "Processing row 650\n",
      "Processing row 700\n",
      "Processing row 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "pos = pos_features(all_books['text_sentence'], common_pos)\n",
    "all_books = pd.concat([pos, all_books])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9534883720930233\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Burgess       0.92      1.00      0.96        96\n",
      "     Carroll       1.00      0.89      0.94        76\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       172\n",
      "   macro avg       0.96      0.95      0.95       172\n",
      "weighted avg       0.96      0.95      0.95       172\n",
      "\n",
      "\n",
      " 0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Burgess       0.69      0.97      0.81        63\n",
      "     Carroll       0.93      0.49      0.64        53\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       116\n",
      "   macro avg       0.81      0.73      0.72       116\n",
      "weighted avg       0.80      0.75      0.73       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "all_books = all_books.fillna(0)\n",
    "X = all_books.loc[(all_books['text_source'] == 'Carroll') | \n",
    "                  (all_books['text_source'] == 'Burgess')\n",
    "                 ]\n",
    "Y = X.loc[:, 'text_source']\n",
    "X = X.drop(['text_sentence', 'text_source', 'is_emma'], axis=1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=2974)\n",
    "\n",
    "lr.fit(X_train, Y_train)\n",
    "print(lr.score(X_train, Y_train))\n",
    "Y_train_pred = lr.predict(X_train)\n",
    "print(classification_report(Y_train, Y_train_pred))\n",
    "\n",
    "print('\\n', lr.score(X_test, Y_test))\n",
    "Y_test_pred = lr.predict(X_test)\n",
    "print(classification_report(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, look at that overfit! The model does a good job of identifying Burgess, but like the previous model, it has a tough time identifying the Carroll sentences. However, it does do better at identifying the Carroll sentences than when comparing to Jane Austen novels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
