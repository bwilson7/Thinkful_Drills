{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NLP to Understand 60 years of Congressional Addresses\n",
    "\n",
    "Every year in January/February the President stands before the U.S. and Congress to give their State of The Union address (SOTU). Normally it is a rambling affair with lots of pauses for *(Applause.)* and *(Laughter.)* and the more than occasional *(Boo.)*. While everyone in the room is intensely analyzing the President and waiting for a political agenda to be revealed there is another group, the general population who just want to hear what the state of the union is. We want to know how our country is fairing in the World and on our home soil, but that is rarely what is delivered.\n",
    "\n",
    "In this notebook I want to dig into the SOTU's from the last 60 years and use unsupervised and supervised NLP workflows to determine if party politics is at the forefront of the addresses. I want to see if the words the presidents use just pander to their party or if they are shooting straight and delivering the State of The Union that the rest of the country wants to hear.\n",
    "\n",
    "As I work through the problem I will use the nltk state_union corpus and spacy module to tokenize the words in the speeches to try and predict first the speaker and secondly their political party. Before gettig to the supervised learning approach of classification, I want to try and cluster the sentences together using tf-idf vectors with SVD to perform latent semantic analysis. Once the components are generated I will use the component scores for each sentence to try and predict the speaker and their political affiliation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import scipy\n",
    "import nltk\n",
    "from nltk.corpus import state_union, stopwords\n",
    "import re\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from collections import Counter\n",
    "\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /Users/brien/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('state_union')\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_load_raw(president, year_lst):\n",
    "    pres = \"\"\n",
    "    for year in year_lst:\n",
    "        pres += state_union.raw('{}-{}.txt'.format(year, president))\n",
    "    print('{}: {} words'.format(president, len(pres)))\n",
    "    return pres\n",
    "\n",
    "def speech_load_sents(president, year_lst):\n",
    "    pres = state_union.sents('{}-{}.txt'.format(year_lst[0], president))[2:]\n",
    "    for year in year_lst[1:]:\n",
    "        pres += state_union.sents('{}-{}.txt'.format(year, president))[2:]\n",
    "    print('{}: {} sentences'.format(president, len(pres)))\n",
    "    return pres\n",
    "\n",
    "def text_cleaner(text):\n",
    "    # getting read of the title, special punctuation and the indicator\n",
    "    # that the president is speaking\n",
    "    text = re.sub(r\"([A-Z]{2,10}\\s)\", '', text)\n",
    "    text = re.sub(r'(PRESIDENT:)', '', text)\n",
    "    pattern = r\"(CLINTON'S|S. TRUMAN'S|D. EISENHOWER'S|F. KENNEDY'S|B. JOHNSON'S|R. FORD'S|CARTER'S|REAGAN'S|H.W. BUSH'S)\"\n",
    "    text = re.sub(pattern, '', text)\n",
    "    text = re.sub(r\"((Applause.)|(Laughter.))\", '', text)\n",
    "    text = re.sub(r'(-|--)',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_.lower()\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(1000)]\n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['speech_sentence'] = sentences['sentence']\n",
    "    df['speaking_president'] = sentences['president']\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['speech_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 10 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "def bag_of_pos(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.pos_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(20)]\n",
    "\n",
    "def amount_punc(sentence):\n",
    "    #selecting only the punctuation in the sentence\n",
    "    all_punc = [token\n",
    "                for token in sentence\n",
    "                if token.is_punct]\n",
    "    #returning the amount of punctuation\n",
    "    return len(all_punc)\n",
    "\n",
    "def pos_features(sentences, common_pos):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_pos)\n",
    "    df['text_sentence'] = sentences\n",
    "    df.loc[:, common_pos] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        pos = [token.pos_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.pos_ in common_pos\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in pos:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Clustering with Tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Raw Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truman: 2325 sentences\n",
      "Eisenhower: 2065 sentences\n",
      "Kennedy: 496 sentences\n",
      "Johnson: 1231 sentences\n",
      "Johnson: 1694 sentences\n",
      "Nixon: 618 sentences\n",
      "Ford: 496 sentences\n",
      "Carter: 411 sentences\n",
      "Reagan: 1634 sentences\n",
      "Bush: 811 sentences\n",
      "Bush: 1223 sentences\n",
      "Clinton: 2612 sentences\n",
      "GWBush: 1215 sentences\n",
      "GWBush: 1809 sentences\n"
     ]
    }
   ],
   "source": [
    "#loading in the raw sentences from the Corpus\n",
    "truman = speech_load_sents('Truman', range(1945, 1951, 1))\n",
    "ike = speech_load_sents('Eisenhower', range(1953, 1960, 1))\n",
    "jfk = speech_load_sents('Kennedy', range(1961, 1963, 1))\n",
    "\n",
    "lbj = speech_load_sents('Johnson', [1964, 1966, 1967, 1968, 1969])\n",
    "#odd year with 2 speeches\n",
    "lbj += state_union.sents('1965-Johnson-1.txt')\n",
    "lbj += state_union.sents('1965-Johnson-2.txt')\n",
    "print('Johnson: {} sentences'.format(len(lbj)))\n",
    "\n",
    "nixon = speech_load_sents('Nixon', range(1970, 1974, 1))\n",
    "ford = speech_load_sents('Ford', range(1975, 1977, 1))\n",
    "carter = speech_load_sents('Carter', range(1978, 1980, 1))\n",
    "reagan = speech_load_sents('Reagan', range(1981, 1988, 1))\n",
    "\n",
    "bush = speech_load_sents('Bush', [1989, 1990, 1992])\n",
    "bush += state_union.sents('1991-Bush-1.txt')\n",
    "bush += state_union.sents('1991-Bush-2.txt')\n",
    "print('Bush: {} sentences'.format(len(bush)))\n",
    "\n",
    "clinton = speech_load_sents('Clinton', range(1993, 2000, 1))\n",
    "\n",
    "gwb = speech_load_sents('GWBush', range(2002, 2006, 1))\n",
    "gwb += state_union.sents('2001-GWBush-1.txt')\n",
    "gwb += state_union.sents('2001-GWBush-2.txt')\n",
    "print('GWBush: {} sentences'.format(len(gwb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sotu_sentence</th>\n",
       "      <th>sotu_president</th>\n",
       "      <th>pol_party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr . Speaker , Mr . President , Members of the...</td>\n",
       "      <td>Truman</td>\n",
       "      <td>DEM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Only yesterday , we laid to rest the mortal re...</td>\n",
       "      <td>Truman</td>\n",
       "      <td>DEM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>At a time like this , words are inadequate .</td>\n",
       "      <td>Truman</td>\n",
       "      <td>DEM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The most eloquent tribute would be a reverent ...</td>\n",
       "      <td>Truman</td>\n",
       "      <td>DEM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yet , in this decisive hour , when world event...</td>\n",
       "      <td>Truman</td>\n",
       "      <td>DEM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sotu_sentence sotu_president pol_party\n",
       "0  Mr . Speaker , Mr . President , Members of the...         Truman       DEM\n",
       "1  Only yesterday , we laid to rest the mortal re...         Truman       DEM\n",
       "2       At a time like this , words are inadequate .         Truman       DEM\n",
       "3  The most eloquent tribute would be a reverent ...         Truman       DEM\n",
       "4  Yet , in this decisive hour , when world event...         Truman       DEM"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presidents = {\n",
    "    'Truman':truman, \n",
    "    'Eisenhower':ike, \n",
    "    'Kennedy':jfk, \n",
    "    'Johnson':lbj, \n",
    "    'Nixon':nixon, \n",
    "    'Ford':ford, \n",
    "    'Carter':carter, \n",
    "    'Reagan':reagan, \n",
    "    'Bush':bush, \n",
    "    'Clinton':clinton, \n",
    "    'GWBush':gwb\n",
    "}\n",
    "\n",
    "party = {\n",
    "    'Truman':'DEM', \n",
    "    'Eisenhower':'REP', \n",
    "    'Kennedy':'DEM', \n",
    "    'Johnson':'DEM', \n",
    "    'Nixon':'REP', \n",
    "    'Ford':'REP', \n",
    "    'Carter':'DEM', \n",
    "    'Reagan':'REP', \n",
    "    'Bush':'REP', \n",
    "    'Clinton':'DEM', \n",
    "    'GWBush':'REP'\n",
    "}\n",
    "pres_sentences = [[' '.join(sentence), pres] for pres in presidents for sentence in presidents[pres]]\n",
    "\n",
    "sentences = pd.DataFrame(pres_sentences, columns=['sotu_sentence', 'sotu_president'])\n",
    "\n",
    "for pres in party:\n",
    "    sentences.loc[sentences['sotu_president'] == pres, 'pol_party'] = party[pres]\n",
    "\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in Training: 11537\n",
      "Number of rows in Test: 3846\n"
     ]
    }
   ],
   "source": [
    "X = sentences.loc[:, 'sotu_sentence']\n",
    "Y = sentences.loc[:, ['sotu_president', 'pol_party']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=6)\n",
    "\n",
    "print('Number of rows in Training:', len(X_train))\n",
    "print('Number of rows in Test:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters on Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Vectorization of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1662\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, # drop words that occur in more than half the time\n",
    "    min_df=0.001, \n",
    "    stop_words='english', # do not include stop words\n",
    "    lowercase=True,\n",
    "    use_idf=True,\n",
    "    norm=u'l2', \n",
    "    smooth_idf=True \n",
    ")\n",
    "\n",
    "sotu_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print('Number of features:', sotu_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words excluded based on vectorizer limiters: 8803\n"
     ]
    }
   ],
   "source": [
    "print('Number of words excluded based on vectorizer limiters:', len(vectorizer.stop_words_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will continue to cooperate with the Federal Reserve Board , seeking a steady policy that ensures price stability without keeping interest rates artificially high or needlessly holding down growth .\n",
      "{'continue': 0.21098489877764123, 'cooperate': 0.3246295181264091, 'federal': 0.1861786313991985, 'reserve': 0.30127398128042426, 'board': 0.3307057280782114, 'seeking': 0.3103425586602814, 'steady': 0.2981272800699197, 'policy': 0.22307052525154417, 'price': 0.24407583976465427, 'stability': 0.27991168258486615, 'keeping': 0.2898832147161841, 'rates': 0.24120298351361513, 'high': 0.22685937155722882, 'growth': 0.22080203737152976}\n"
     ]
    }
   ],
   "source": [
    "sotu_tfidf_csr = sotu_tfidf.tocsr()\n",
    "\n",
    "n = sotu_tfidf_csr.shape[0]\n",
    "tfidf_by_sent = [{} for _ in range(0,n)]\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "for i,j in zip(*sotu_tfidf_csr.nonzero()):\n",
    "    tfidf_by_sent[i][words[j]] = sotu_tfidf_csr[i, j]\n",
    "\n",
    "print(X_train.iloc[20])\n",
    "print(tfidf_by_sent[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature/Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 57.179293631926285\n",
      "\n",
      "Component 0:\n",
      "sotu_sentence\n",
      "( Applause .)    0.999801\n",
      "( Applause .)    0.999801\n",
      "( Applause .)    0.999801\n",
      "( Applause .)    0.999801\n",
      "( Applause .)    0.999801\n",
      "( Applause .)    0.999801\n",
      "( Applause .)    0.999801\n",
      "( Applause .)    0.999801\n",
      "( Applause .)    0.999801\n",
      "( Applause .)    0.999801\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Component 1:\n",
      "sotu_sentence\n",
      "It is a time to build , to build the America within reach , an America where everybody has a chance to get ahead with hard work ; where every citizen can live in a safe community ; where families are strong , schools are good , and all our young people can go on to college ; an America where scientists find cures for diseases , from diabetes to Alzheimer ' s to AIDS ; an America where every child can stretch a hand across a keyboard and reach every book ever written , every painting ever painted , every symphony ever composed ; where government provides opportunity and citizens honor the responsibility to give something back to their communities ; an America which leads the world to new heights of peace and prosperity .                                                                                                                                                                                                                                                                                                                                                                                                                                                   0.383269\n",
      "Most important , because of the beginnings that have been made , we can say today that this year 1972 can be the year in which America may make the greatest progress in 25 years toward achieving our goal of being at peace with all the nations of the world .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           0.370503\n",
      "America continues to provide a better and more abundant life for more of its people than any other nation in the world .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    0.369563\n",
      "But until world conditions permit , and until peace is assured , America ' s might -- and America ' s bravest sons who wear our Nation ' s uniform -- must continue to stand guard for all of us -- as they gallantly do tonight in Vietnam and other places in the world .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 0.361462\n",
      "So I ask you now in the Congress and in the country to join with me in expressing and fulfilling that faith in working for a nation , a nation that is free from want and a world that is free from hate -- a world of peace and justice , and freedom and abundance , for our time and for all time to come .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0.361083\n",
      "In this year 1979 , nothing is more important than that the Congress and the people of the United States resolve to continue with me on that path of nuclear arms control and world peace .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 0.357237\n",
      "And our challenge today is to take this democratic system of ours , a system second to none , and make it better : a better America , where there ' s a job for everyone who wants one ; where women working outside the home can be confident their children are in safe and loving care and where government works to expand child - care alternatives for parents ; where we reconcile the needs of a clean environment and a strong economy ; where `` Made in the USA '' is recognized around the world as the symbol of quality and progress ; where every one of us enjoys the same opportunities to live , to work , and to contribute to society and where , for the first time , the American mainstream includes all of our disabled citizens ; where everyone has a roof over his head and where the homeless get the help they need to live in dignity ; where our schools challenge and support our kids and our teachers and where all of them make the grade ; where every street , every city , every school , and every child is drug - free ; and finally , where no American is forgotten -- our hearts go out to our hostages who are ceaselessly on our minds and in our efforts .    0.356848\n",
      "The people of this great country have a right to expect that the Congress and the President will work in closest cooperation with one objective - the welfare of the people of this Nation as a whole .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     0.355741\n",
      "The civilized world is rallying to America ' s side .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       0.355537\n",
      "America and the world will not be blackmailed .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             0.355537\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Component 2:\n",
      "sotu_sentence\n",
      "America and the world will not be blackmailed .                           0.614706\n",
      "The civilized world is rallying to America ' s side .                     0.614706\n",
      "And so must we , here and all around the world .                          0.517480\n",
      "They are the world ' s finest .                                           0.517480\n",
      "We are the world ' s only superpower .                                    0.517480\n",
      "And that was our world , until now .                                      0.517480\n",
      "He had the world at his feet .                                            0.517480\n",
      "And the whole world will rejoice .                                        0.517480\n",
      "The whole world is watching to see how we respond .                       0.508953\n",
      "For this , too , bears on whether or not we can compete in the world .    0.504272\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Component 3:\n",
      "sotu_sentence\n",
      "The American people aren ' t impressed by gimmicks ; they ' re smarter on this score than all of us in this room .    0.603487\n",
      "We the people .                                                                                                       0.603467\n",
      "We the people .                                                                                                       0.603467\n",
      "We the people .                                                                                                       0.603467\n",
      "If ever there was a people who sought more than mere abundance , it is our people .                                   0.579862\n",
      "The people of that Territory have earned that status .                                                                0.567232\n",
      "The people are watching and waiting .                                                                                 0.566453\n",
      "We are a confident people and a hardworking people , a decent and a compassionate people , and so we will remain .    0.532780\n",
      "more people a chance to do something , we can have government that truly is by the people .                           0.520076\n",
      "The radical extremism of their Government has isolated the Chinese people behind their own borders .                  0.518848\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Component 4:\n",
      "sotu_sentence\n",
      "And that ' s what America is all about .                                                                                                                                             0.692155\n",
      "That is what America will do .                                                                                                                                                       0.692155\n",
      "America is on the move !                                                                                                                                                             0.692155\n",
      "And their vigilance is protecting America .                                                                                                                                          0.681760\n",
      "America has traditionally been generous in caring for the disabled -- and the widow and the orphan of the fallen .                                                                   0.674145\n",
      "I have a very deep belief in America ' s capabilities .                                                                                                                              0.607984\n",
      "These investments will keep America competitive .                                                                                                                                    0.607689\n",
      "And on your uniform , once again , you will carry America ' s flag , marking the unbroken connection between the deeds of America ' s past and the daring of America ' s future .    0.598279\n",
      "America will never forget their sacrifices .                                                                                                                                         0.571548\n",
      "We can develop America ' s next frontier .                                                                                                                                           0.540785\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Feature reduction, input for svd is number of clusters\n",
    "svd = TruncatedSVD(400)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(sotu_tfidf)\n",
    "\n",
    "variance_explained = svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "sents_by_component = pd.DataFrame(X_train_lsa, index=X_train)\n",
    "for i in range(5):\n",
    "    print('\\nComponent {}:'.format(i))\n",
    "    print(sents_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the general themes of the sentences for the top 5 components from the SVD.\n",
    "\n",
    "comp 0: applause\n",
    "comp 1: rallying, building, strength, improvement America\n",
    "comp 2: world comments\n",
    "comp 3: sentences about people\n",
    "comp 4: 'Merica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Modeling based on Unsupervised Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________Training Statistics__________\n",
      "[[3570 2056]\n",
      " [1943 3968]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DEM       0.65      0.63      0.64      5626\n",
      "         REP       0.66      0.67      0.66      5911\n",
      "\n",
      "   micro avg       0.65      0.65      0.65     11537\n",
      "   macro avg       0.65      0.65      0.65     11537\n",
      "weighted avg       0.65      0.65      0.65     11537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l1')\n",
    "\n",
    "X_train_unsup = sents_by_component\n",
    "Y_train_unsup = Y_train.loc[:, 'pol_party']\n",
    "\n",
    "lr.fit(X_train_unsup, Y_train_unsup)\n",
    "\n",
    "Y_train_pred_unsup = lr.predict(X_train_unsup)\n",
    "\n",
    "print('__________Training Statistics__________')\n",
    "print(confusion_matrix(Y_train_unsup, Y_train_pred_unsup))\n",
    "print(classification_report(Y_train_unsup, Y_train_pred_unsup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason I could not choose 'elasticnet' for the logistic regression model. It says that the only penalties available are l1 or l2, even though elasticnet is a valid option based on the sklearn docs. Could be an sklearn version issue, but I am not sure. Given that I wanted to try the SGDClassifier as it did have an option for elasticnet and given the number of features, I thought that would be the best way to handle classification. The one thing that I do want to do with the logistic regression model is set it up for multiclass predictions to see how well the unsupervised clusters can identify the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________Training Statistics__________\n",
      "[[ 178    1  334   91    2   72   99    2    5   90   61]\n",
      " [  15   17   77   45    2   16   21    1    2   51   58]\n",
      " [  51    1 1432   74    1  114  114    2    6  106   82]\n",
      " [  19    3  147  867    1   34   90    3    4   73  333]\n",
      " [   9    2  105   61   20   22   38    1    5   38   48]\n",
      " [  39    1  313   75    2  706   75    1    6   88   60]\n",
      " [  29    1  280  160    2   52  487    2    4   68  161]\n",
      " [   8    0   54  119    0   15   46   24    1   30   84]\n",
      " [  22    1  111   71    1   13   63    2   68   46   48]\n",
      " [  52    4  338  134    3   70   98    1    7  420  114]\n",
      " [  19    1  134  275    1   40   77    2    9   39 1114]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bush       0.40      0.19      0.26       935\n",
      "      Carter       0.53      0.06      0.10       305\n",
      "     Clinton       0.43      0.72      0.54      1983\n",
      "  Eisenhower       0.44      0.55      0.49      1574\n",
      "        Ford       0.57      0.06      0.10       349\n",
      "      GWBush       0.61      0.52      0.56      1366\n",
      "     Johnson       0.40      0.39      0.40      1246\n",
      "     Kennedy       0.59      0.06      0.11       381\n",
      "       Nixon       0.58      0.15      0.24       446\n",
      "      Reagan       0.40      0.34      0.37      1241\n",
      "      Truman       0.52      0.65      0.58      1711\n",
      "\n",
      "   micro avg       0.46      0.46      0.46     11537\n",
      "   macro avg       0.50      0.34      0.34     11537\n",
      "weighted avg       0.48      0.46      0.43     11537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', solver='newton-cg', multi_class='multinomial')\n",
    "\n",
    "X_train_unsup = sents_by_component\n",
    "Y_train_unsup = Y_train.loc[:, 'sotu_president']\n",
    "\n",
    "lr.fit(X_train_unsup, Y_train_unsup)\n",
    "\n",
    "Y_train_pred_unsup_mcl = lr.predict(X_train_unsup)\n",
    "\n",
    "print('__________Training Statistics__________')\n",
    "print(confusion_matrix(Y_train_unsup, Y_train_pred_unsup_mcl))\n",
    "print(classification_report(Y_train_unsup, Y_train_pred_unsup_mcl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model actually doesn't do too bad with classifying the presidents in regards to the precision, but there are a lot of issues with class imbalance in the dataset. Some presidents do seem to do better than others. GWBush has the best performance followed by JFK. Both presidents were known for their speaking abilities, for bad and good reasons respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    6.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   32.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   24.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   27.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   21.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.6579478988568594, 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', multi_class='multinomial', class_weight='balanced')\n",
    "\n",
    "params = {\n",
    "    'solver':Categorical(['newton-cg', 'sag', 'saga', 'lbfgs']),\n",
    "    'C':Real(0.001, 1, 'uniform')\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    lr,\n",
    "    params,\n",
    "    cv=5,\n",
    "    n_iter=7,\n",
    "    random_state=45,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "opt.fit(X_train_unsup, Y_train_unsup)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________Training Statistics__________\n",
      "[[ 155    0  356   99    2   71   96    1    2   90   63]\n",
      " [  14    5   85   47    0   14   22    1    3   51   63]\n",
      " [  42    0 1456   76    1  108  104    1    5  102   88]\n",
      " [  14    3  159  867    0   30   86    1    2   63  349]\n",
      " [   8    1  113   65    9   18   36    0    3   38   58]\n",
      " [  33    0  336   77    2  687   73    0    6   86   66]\n",
      " [  23    0  305  161    2   45  472    1    3   68  166]\n",
      " [   7    0   61  124    0   14   43   12    0   33   87]\n",
      " [  20    0  125   78    1   13   68    0   43   47   51]\n",
      " [  41    2  363  146    2   62   95    1    3  408  118]\n",
      " [  16    1  144  281    1   35   80    1    4   36 1112]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bush       0.42      0.17      0.24       935\n",
      "      Carter       0.42      0.02      0.03       305\n",
      "     Clinton       0.42      0.73      0.53      1983\n",
      "  Eisenhower       0.43      0.55      0.48      1574\n",
      "        Ford       0.45      0.03      0.05       349\n",
      "      GWBush       0.63      0.50      0.56      1366\n",
      "     Johnson       0.40      0.38      0.39      1246\n",
      "     Kennedy       0.63      0.03      0.06       381\n",
      "       Nixon       0.58      0.10      0.17       446\n",
      "      Reagan       0.40      0.33      0.36      1241\n",
      "      Truman       0.50      0.65      0.57      1711\n",
      "\n",
      "   micro avg       0.45      0.45      0.45     11537\n",
      "   macro avg       0.48      0.32      0.31     11537\n",
      "weighted avg       0.47      0.45      0.42     11537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', solver='newton-cg', multi_class='multinomial', C=0.6579)\n",
    "\n",
    "X_train_unsup = sents_by_component\n",
    "Y_train_unsup = Y_train.loc[:, 'sotu_president']\n",
    "\n",
    "lr.fit(X_train_unsup, Y_train_unsup)\n",
    "\n",
    "Y_train_pred_unsup_mcl = lr.predict(X_train_unsup)\n",
    "\n",
    "print('__________Training Statistics__________')\n",
    "print(confusion_matrix(Y_train_unsup, Y_train_pred_unsup_mcl))\n",
    "print(classification_report(Y_train_unsup, Y_train_pred_unsup_mcl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning helped smooth out some of the precisions and even increased GWBush and Kennedy. Some of the smoothing is likely due to changing the class_weight to balanced. Since performance is still low overall, let's take a look at the SGD Classifier and see if we can use the elasticnet penalty to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   27.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   32.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   40.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   44.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   41.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 4.015624554416855e-05, 'l1_ratio': 0.07108300499461508, 'learning_rate': 'constant'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "sgdc = SGDClassifier(\n",
    "    loss='log',\n",
    "    penalty='elasticnet',\n",
    "    tol=0.00001,\n",
    "    n_iter_no_change=10,\n",
    "    eta0=0.001,\n",
    "    max_iter=250\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'learning_rate':Categorical(['optimal', 'constant', 'adaptive']),\n",
    "    'alpha':Real(1e-10, 0.1, 'uniform'),\n",
    "    'l1_ratio':Real(0, 1, 'uniform')\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    sgdc,\n",
    "    params,\n",
    "    cv=5,\n",
    "    n_iter=7,\n",
    "    random_state=54,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "opt.fit(X_train_unsup, Y_train_unsup)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BayesSearch params chose an l1_ratio close to 0 which is effectively an L2 penalty so I assume that the results of the SGDClassifier will be pretty close to the LR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________Training Statistics__________\n",
      "1000\n",
      "[[ 160    1  339  100    2   75   94    2    7   91   64]\n",
      " [  16    9   85   46    0   15   21    1    3   49   60]\n",
      " [  45    1 1440   77    1  111  108    2    6  102   90]\n",
      " [  16    3  158  844    0   36   84    2    2   70  359]\n",
      " [   9    1  108   61   13   22   37    1    5   40   52]\n",
      " [  32    1  318   79    2  698   74    0    7   87   68]\n",
      " [  24    0  290  159    2   50  474    1    4   72  170]\n",
      " [   8    0   58  123    0   15   44   17    1   31   84]\n",
      " [  20    1  118   77    1   12   64    0   59   45   49]\n",
      " [  45    4  349  142    2   69   93    1    5  417  114]\n",
      " [  18    1  142  274    1   38   74    1    6   38 1118]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bush       0.41      0.17      0.24       935\n",
      "      Carter       0.41      0.03      0.06       305\n",
      "     Clinton       0.42      0.73      0.53      1983\n",
      "  Eisenhower       0.43      0.54      0.47      1574\n",
      "        Ford       0.54      0.04      0.07       349\n",
      "      GWBush       0.61      0.51      0.56      1366\n",
      "     Johnson       0.41      0.38      0.39      1246\n",
      "     Kennedy       0.61      0.04      0.08       381\n",
      "       Nixon       0.56      0.13      0.21       446\n",
      "      Reagan       0.40      0.34      0.37      1241\n",
      "      Truman       0.50      0.65      0.57      1711\n",
      "\n",
      "   micro avg       0.45      0.45      0.45     11537\n",
      "   macro avg       0.48      0.32      0.32     11537\n",
      "weighted avg       0.47      0.45      0.42     11537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "sgdc = SGDClassifier(\n",
    "    loss='log',\n",
    "    penalty='elasticnet',\n",
    "    tol=0.00001,\n",
    "    n_iter_no_change=10,\n",
    "    eta0=0.001,\n",
    "    learning_rate='constant',\n",
    "    alpha=4e-5,\n",
    "    l1_ratio=0.071\n",
    ")\n",
    "\n",
    "sgdc.fit(X_train_unsup, Y_train_unsup)\n",
    "\n",
    "Y_train_pred_unsup_sgdc = sgdc.predict(X_train_unsup)\n",
    "\n",
    "print('__________Training Statistics__________')\n",
    "print(sgdc.n_iter_)\n",
    "print(confusion_matrix(Y_train_unsup, Y_train_pred_unsup_sgdc))\n",
    "print(classification_report(Y_train_unsup, Y_train_pred_unsup_sgdc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like SGD with an elasticnet penalty didn't quite perform as well as Logistic Regression with an L2 penalty. However, they are very close to one another in performance. \n",
    "\n",
    "One last thing... lets see if the tuned SGD model can predict party affiliations any better than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________Training Statistics__________\n",
      "474\n",
      "[[3533 2093]\n",
      " [1885 4026]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DEM       0.65      0.63      0.64      5626\n",
      "         REP       0.66      0.68      0.67      5911\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     11537\n",
      "   macro avg       0.66      0.65      0.65     11537\n",
      "weighted avg       0.66      0.66      0.65     11537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_unsup = sents_by_component\n",
    "Y_train_unsup = Y_train.loc[:, 'pol_party']\n",
    "\n",
    "sgdc = SGDClassifier(\n",
    "    loss='log',\n",
    "    penalty='elasticnet',\n",
    "    tol=0.00001,\n",
    "    n_iter_no_change=10,\n",
    "    eta0=0.001,\n",
    "    learning_rate='constant',\n",
    "    alpha=4e-5,\n",
    "    l1_ratio=0.071\n",
    ")\n",
    "\n",
    "sgdc.fit(X_train_unsup, Y_train_unsup)\n",
    "\n",
    "Y_train_pred_unsup_sgdc = sgdc.predict(X_train_unsup)\n",
    "\n",
    "print('__________Training Statistics__________')\n",
    "print('Number of Iterations:', sgdc.n_iter_)\n",
    "print(confusion_matrix(Y_train_unsup, Y_train_pred_unsup_sgdc))\n",
    "print(classification_report(Y_train_unsup, Y_train_pred_unsup_sgdc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is similar to the LR model that was done previously, which isn't too surprising again because of the penalty being essentially the same.\n",
    "\n",
    "It's time to repeat the process for the test set and run the LR model with an L2 penalty to predict authorship and political affiliation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Vectorization of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1662\n",
      "Number of words excluded based on vectorizer limiters: 4677\n",
      "This is not the sole responsibility of any one branch of our government .\n",
      "{'responsibility': 0.5418620140275172, 'branch': 0.7476104822337042, 'government': 0.3840105787713807}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, # drop words that occur in more than half the time\n",
    "    min_df=0.001, \n",
    "    stop_words='english', # do not include stop words\n",
    "    lowercase=True,\n",
    "    use_idf=True,\n",
    "    norm=u'l2', \n",
    "    smooth_idf=True \n",
    ")\n",
    "\n",
    "sotu_tfidf_test = vectorizer.fit_transform(X_test)\n",
    "\n",
    "print('Number of features:', sotu_tfidf.get_shape()[1])\n",
    "print('Number of words excluded based on vectorizer limiters:', len(vectorizer.stop_words_))\n",
    "\n",
    "sotu_tfidf_csr_test = sotu_tfidf_test.tocsr()\n",
    "\n",
    "n = sotu_tfidf_csr_test.shape[0]\n",
    "tfidf_by_sent_test = [{} for _ in range(0,n)]\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "for i,j in zip(*sotu_tfidf_csr_test.nonzero()):\n",
    "    tfidf_by_sent_test[i][words[j]] = sotu_tfidf_csr_test[i, j]\n",
    "\n",
    "print(X_test.iloc[20])\n",
    "print(tfidf_by_sent_test[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature/Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 60.34728350241241\n",
      "\n",
      "Component 0:\n",
      "sotu_sentence\n",
      "( Applause .)    0.999223\n",
      "( Applause .)    0.999223\n",
      "( Applause .)    0.999223\n",
      "( Applause .)    0.999223\n",
      "( Applause .)    0.999223\n",
      "( Applause .)    0.999223\n",
      "( Applause .)    0.999223\n",
      "( Applause .)    0.999223\n",
      "( Applause .)    0.999223\n",
      "( Applause .)    0.999223\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Component 1:\n",
      "sotu_sentence\n",
      "That is why my call upon the Congress today is for a high statesmanship , so that in the years to come Americans will look back and say because it withstood the intense pressures of a political year , and achieved such great good for the American people and for the future of this Nation , this was truly a great Congress .                                                                                                                                                                                           0.387325\n",
      "I thank the Vice President for his leadership and the Congress for its support in building a Government that is leaner , more flexible , a catalyst for new ideas , and most of all , a Government that gives the American people the tools they need to make the most of their own lives .                                                                                                                                                                                                                                   0.355305\n",
      "This nation , this idea called America , was and always will be a new world -- our new world .                                                                                                                                                                                                                                                                                                                                                                                                                                0.352200\n",
      "This year ' s budget provides over $ 2 billion in new spending to protect our environment , with over $ 1 billion for global change research , and a new initiative I call America the Beautiful to expand our national parks and wildlife preserves that improve recreational facilities on public lands , and something else , something that will help keep this country clean from our forestland to the inner cities and keep America beautiful for generations to come : the money to plant a billion trees a year .    0.325242\n",
      "If the Congress and the American people will work with me to attain these targets , they will be achieved and will be surpassed .                                                                                                                                                                                                                                                                                                                                                                                             0.318310\n",
      "Now let us join in making 1972 a year of action on them , action by the Congress , for the Nation and for the people of America .                                                                                                                                                                                                                                                                                                                                                                                             0.314231\n",
      "And a nation with a strong government and a weak people is an empty shell .                                                                                                                                                                                                                                                                                                                                                                                                                                                   0.313820\n",
      "Today , when we are the richest and strongest nation in the world , let it not be recorded that we lack the moral and spiritual idealism which made us the hope of the world at the time of our birth .                                                                                                                                                                                                                                                                                                                       0.313688\n",
      "But we can ' t be proud of the fact that we ' re the only wealthy country in the world that has a smaller percentage of the work force and their children with health insurance today than we did 10 years ago , the last time we were the most productive economy in the world .                                                                                                                                                                                                                                             0.310328\n",
      "So let us pledge together to go forward togetherĄXby achieving these goals to give America the foundation today for a new greatness tomorrow and in all the years to come , and in so doing to make this the greatest Congress in the history of this great and good country .                                                                                                                                                                                                                                                0.309434\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Component 2:\n",
      "sotu_sentence\n",
      "We must not be the world ' s policeman .                                                                                                                                                                   0.459299\n",
      "Our objective in the world is peace .                                                                                                                                                                      0.420644\n",
      "[ It ] is foremost among the nations of the world in the search for peace .\"                                                                                                                               0.395358\n",
      "This is the richest nation in the world .                                                                                                                                                                  0.380534\n",
      "We are at peace , and we are a force for peace and freedom throughout the world .                                                                                                                          0.361728\n",
      "This statue is more than a landmark ; it is a symbolĄXa symbol of what America has meant to the world .                                                                                                    0.343118\n",
      "Today , when we are the richest and strongest nation in the world , let it not be recorded that we lack the moral and spiritual idealism which made us the hope of the world at the time of our birth .    0.341588\n",
      "The vast majority of the nations of the world have chosen to work together to achieve , on a cooperative basis , world security and world prosperity .                                                     0.340588\n",
      "For two decades America has committed itself against the tyranny of want and ignorance in the world that threatens the peace .                                                                             0.339646\n",
      "All over the world , people are being torn asunder by racial , ethnic , and religious conflicts that fuel fanaticism and terror .                                                                          0.326612\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Component 3:\n",
      "sotu_sentence\n",
      "It has already been delivered by the American people .                                                                               0.599370\n",
      "People are worried .                                                                                                                 0.590775\n",
      "We the people .                                                                                                                      0.590775\n",
      "We the people .                                                                                                                      0.590775\n",
      "People have to take their kids to get immunized .                                                                                    0.530693\n",
      "What government can do alone is limited , but the potential of the American people knows no limits .                                 0.517975\n",
      "We are a good people , a generous people .                                                                                           0.472492\n",
      "The American people must prosper in the global economy .                                                                             0.447795\n",
      "If the Congress and the American people will work with me to attain these targets , they will be achieved and will be surpassed .    0.445806\n",
      "In this hemisphere , every government but one is freely chosen by its people .                                                       0.437627\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Component 4:\n",
      "sotu_sentence\n",
      "Now let us do the same .                                                         0.722356\n",
      "I will not let it be repealed .                                                  0.722356\n",
      "Now we must let our spirits soar again .                                         0.722356\n",
      "Let ' s do whatever we have to do to get something done .                        0.722356\n",
      "Let ' s be frank .                                                               0.722356\n",
      "Let ' s go to work and get this done together .                                  0.684676\n",
      "Yet , let ' s be honest .                                                        0.660211\n",
      "Let ' s stick to facts !                                                         0.653682\n",
      "So , let us take the new openness seriously , but let ' s also be realistic .    0.648386\n",
      "Let me say this .                                                                0.585830\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Feature reduction, input for svd is number of clusters\n",
    "svd = TruncatedSVD(400)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_test_lsa = lsa.fit_transform(sotu_tfidf_test)\n",
    "\n",
    "variance_explained = svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "sents_by_component_test = pd.DataFrame(X_test_lsa, index=X_test)\n",
    "for i in range(5):\n",
    "    print('\\nComponent {}:'.format(i))\n",
    "    print(sents_by_component_test.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component themes for the top 5 components in the test set. Aside from the applause component none of them are the same as the training set. This isn't surprising considering the tfidf scores were done separately. The term and document frequencies for the training and test will be different since the dataset sizes are different. On top of that, since speech timings could affect the content covered, the train/test split altered how the clusters will turn out.\n",
    "\n",
    "Comp 0: Applause\n",
    "Comp 1: Strength and action by the nation and congress\n",
    "Comp 2: ummm 'World Peace!'\n",
    "Comp 3: people, people, people\n",
    "Comp 4: statements of togetherness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Modeling on Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_unsup = sents_by_component_test\n",
    "Y_test_unsup = Y_test.loc[:, 'sotu_president']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    8.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    6.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    6.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.6579478988568594, 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', multi_class='multinomial', class_weight='balanced')\n",
    "\n",
    "params = {\n",
    "    'solver':Categorical(['newton-cg', 'sag', 'saga', 'lbfgs']),\n",
    "    'C':Real(0.001, 1, 'uniform')\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    lr,\n",
    "    params,\n",
    "    cv=5,\n",
    "    n_iter=7,\n",
    "    random_state=45,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "opt.fit(X_test_unsup, Y_test_unsup)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________Training Statistics__________\n",
      "[[126  19  27   6  23  14  23  14  19   9   8]\n",
      " [  1  79   2   2   8   1   2   4   3   2   2]\n",
      " [ 56  34 290  17  25  45  46  26  39  30  21]\n",
      " [ 22  21  13 229  21   9  24  46  20  21  65]\n",
      " [  7   7   2   0  96   3   7   7  10   1   7]\n",
      " [ 31  28  36  13  26 222  19  16  23  20   9]\n",
      " [ 30  28  26  29  26  15 182  31  27  25  29]\n",
      " [  1   1   1   4   2   1   5  92   5   0   3]\n",
      " [  5   7   4   6   4   3   8   6 114   8   7]\n",
      " [ 28  31  41  24  30  11  22  21  26 142  17]\n",
      " [ 11  30  16  69  34  11  29  37  31  13 333]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bush       0.40      0.44      0.42       288\n",
      "      Carter       0.28      0.75      0.40       106\n",
      "     Clinton       0.63      0.46      0.53       629\n",
      "  Eisenhower       0.57      0.47      0.51       491\n",
      "        Ford       0.33      0.65      0.43       147\n",
      "      GWBush       0.66      0.50      0.57       443\n",
      "     Johnson       0.50      0.41      0.45       448\n",
      "     Kennedy       0.31      0.80      0.44       115\n",
      "       Nixon       0.36      0.66      0.47       172\n",
      "      Reagan       0.52      0.36      0.43       393\n",
      "      Truman       0.66      0.54      0.60       614\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      3846\n",
      "   macro avg       0.47      0.55      0.48      3846\n",
      "weighted avg       0.55      0.50      0.50      3846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(\n",
    "    penalty='l2', \n",
    "    multi_class='multinomial', \n",
    "    class_weight='balanced',\n",
    "    C=0.657,\n",
    "    solver='newton-cg'\n",
    ")\n",
    "\n",
    "lr.fit(X_test_unsup, Y_test_unsup)\n",
    "\n",
    "Y_test_pred_unsup_mcl = lr.predict(X_test_unsup)\n",
    "\n",
    "print('__________Training Statistics__________')\n",
    "print(confusion_matrix(Y_test_unsup, Y_test_pred_unsup_mcl))\n",
    "print(classification_report(Y_test_unsup, Y_test_pred_unsup_mcl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiclass prediction of the speaker was less consistent than the training set and overall performed worse. This does make sense considering the component score for each sentence is different and derived from different tfidf scores. It makes sense that the training set overfits to the speaker in this case when you consider that the frequencies of each word will be different from training to test.\n",
    "\n",
    "If the tfidf scores and sentence vectors were done on the whole dataset and then broken into training and test, then I think the multiclass prediction and clusters would have been more stable. Let's check on the party affiliations before moving onto solely supervised techniques for speaker and party predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________Training Statistics__________\n",
      "[[1325  587]\n",
      " [ 638 1296]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DEM       0.67      0.69      0.68      1912\n",
      "         REP       0.69      0.67      0.68      1934\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      3846\n",
      "   macro avg       0.68      0.68      0.68      3846\n",
      "weighted avg       0.68      0.68      0.68      3846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_unsup = sents_by_component_test\n",
    "Y_test_unsup = Y_test.loc[:, 'pol_party']\n",
    "\n",
    "lr = LogisticRegression(penalty='l1')\n",
    "\n",
    "lr.fit(X_test_unsup, Y_test_unsup)\n",
    "\n",
    "Y_test_pred_unsup = lr.predict(X_test_unsup)\n",
    "\n",
    "print('__________Training Statistics__________')\n",
    "print(confusion_matrix(Y_test_unsup, Y_test_pred_unsup))\n",
    "print(classification_report(Y_test_unsup, Y_test_pred_unsup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and recall are very similar to training and shows no overfit. I think the party prediction works better than the speaker prediction because there isn't a class imbalance, party policy lines and speaking points have remained relatively consistent over time, and a presidents ability to please their party probably takes over a good majority of their SOTU address. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Modeling sans clusters\n",
    "\n",
    "I think the next step is to try predicting the political affiliations without the SVD clusters and latent semantic analysis (LSA). Considering the lemma, POS, and punc frequency worked so well with identifying Clinton of GWB in an earlier analysis of the same dataset, I want to try it with the larger dataset to predict political affiliations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Some presidents have multiple speeches recorded in the same year for various reasons. GWBush did after the September 11th attacks, LBJ gave a special session talk for equal voting rights, and Bush Sr. gave a special session talk on the end of the Gulf War. While these aren't technically State of The Union (SOTU) addresses, they do represent the presidents personal affiliations and are representative of how they handled their time in office.\n",
    "\n",
    "Due to the total size of the dataset being > 1.5 million words, I will likely need to take subsets of the speeches for computational resource considerations. As well, I will need to balance the classes some and make sure each president is properly represented in the dataset. At the end of the day we are trying to group presidents out based on their words, not if they were removed from office & the VP took over with only 3 years to speek (looking at you Gerald & Nixon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truman: 301567 words\n",
      "Eisenhower: 266595 words\n",
      "Kennedy: 75014 words\n",
      "Johnson: 144866 words\n",
      "Johnson: 190589 words\n",
      "Nixon: 83599 words\n",
      "Ford: 54372 words\n",
      "Carter: 46053 words\n",
      "Reagan: 188406 words\n",
      "Bush: 78229 words\n",
      "Bush: 117973 words\n",
      "Clinton: 293677 words\n",
      "GWBush: 119011 words\n",
      "GWBush: 163861 words\n"
     ]
    }
   ],
   "source": [
    "truman = speech_load('Truman', range(1945, 1951, 1))\n",
    "ike = speech_load('Eisenhower', range(1953, 1960, 1))\n",
    "jfk = speech_load('Kennedy', range(1961, 1963, 1))\n",
    "\n",
    "lbj = speech_load('Johnson', [1964, 1966, 1967, 1968, 1969])\n",
    "#odd year with 2 speeches\n",
    "lbj += state_union.raw('1965-Johnson-1.txt')\n",
    "lbj += state_union.raw('1965-Johnson-2.txt')\n",
    "print('Johnson: {} words'.format(len(lbj)))\n",
    "\n",
    "nixon = speech_load('Nixon', range(1970, 1974, 1))\n",
    "ford = speech_load('Ford', range(1975, 1977, 1))\n",
    "carter = speech_load('Carter', range(1978, 1980, 1))\n",
    "reagan = speech_load('Reagan', range(1981, 1988, 1))\n",
    "\n",
    "bush = speech_load('Bush', [1989, 1990, 1992])\n",
    "bush += state_union.raw('1991-Bush-1.txt')\n",
    "bush += state_union.raw('1991-Bush-2.txt')\n",
    "print('Bush: {} words'.format(len(bush)))\n",
    "\n",
    "clinton = speech_load('Clinton', range(1993, 2000, 1))\n",
    "\n",
    "gwb = speech_load('GWBush', range(2002, 2006, 1))\n",
    "gwb += state_union.raw('2001-GWBush-1.txt')\n",
    "gwb += state_union.raw('2001-GWBush-2.txt')\n",
    "print('GWBush: {} words'.format(len(gwb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning & Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning all the speeches text & removing the titles\n",
    "truman = text_cleaner(truman)\n",
    "ike = text_cleaner(ike)\n",
    "jfk = text_cleaner(jfk)\n",
    "lbj = text_cleaner(lbj)\n",
    "nixon = text_cleaner(nixon)\n",
    "ford = text_cleaner(ford)\n",
    "carter = text_cleaner(carter)\n",
    "reagan = text_cleaner(reagan)\n",
    "bush = text_cleaner(bush)\n",
    "clinton = text_cleaner(clinton)\n",
    "gwb = text_cleaner(gwb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell could take a couple minutes to run\n",
    "truman_doc = nlp(truman)\n",
    "ike_doc = nlp(ike)\n",
    "jfk_doc = nlp(jfk)\n",
    "lbj_doc = nlp(lbj)\n",
    "nixon_doc = nlp(nixon)\n",
    "ford_doc = nlp(ford)\n",
    "carter_doc = nlp(carter)\n",
    "reagan_doc = nlp(reagan)\n",
    "bush_doc = nlp(bush)\n",
    "clinton_doc = nlp(clinton)\n",
    "gwb_doc = nlp(gwb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Grouping for Token Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truman Sentences: 2353\n",
      "Ike Sentences: 2080\n",
      "JFK Sentences: 511\n",
      "LBJ Sentences: 1715\n",
      "Nixon Sentences: 648\n",
      "Ford Sentences: 505\n",
      "Carter Sentences: 415\n",
      "Reagan Sentences: 1680\n",
      "Bush Sentences: 1268\n",
      "Clinton Sentences: 2689\n",
      "GWBush Sentences: 1577\n"
     ]
    }
   ],
   "source": [
    "truman_sents = [[sents, 'Truman'] for sents in truman_doc.sents]\n",
    "ike_sents = [[sents, 'Eisenhower'] for sents in ike_doc.sents]\n",
    "jfk_sents = [[sents, 'Kennedy'] for sents in jfk_doc.sents]\n",
    "lbj_sents = [[sents, 'Johnson'] for sents in lbj_doc.sents]\n",
    "nixon_sents = [[sents, 'Nixon'] for sents in nixon_doc.sents]\n",
    "ford_sents = [[sents, 'Ford'] for sents in ford_doc.sents]\n",
    "carter_sents = [[sents, 'Carter'] for sents in carter_doc.sents]\n",
    "reagan_sents = [[sents, 'Reagan'] for sents in reagan_doc.sents]\n",
    "bush_sents = [[sents, 'Bush'] for sents in bush_doc.sents]\n",
    "clinton_sents = [[sents, 'Clinton'] for sents in clinton_doc.sents]\n",
    "gwb_sents = [[sents, 'GWBush'] for sents in gwb_doc.sents]\n",
    "\n",
    "print('Truman Sentences:', len(truman_sents))\n",
    "print('Ike Sentences:', len(ike_sents))\n",
    "print('JFK Sentences:', len(jfk_sents))\n",
    "print('LBJ Sentences:', len(lbj_sents))\n",
    "print('Nixon Sentences:', len(nixon_sents))\n",
    "print('Ford Sentences:', len(ford_sents))\n",
    "print('Carter Sentences:', len(carter_sents))\n",
    "print('Reagan Sentences:', len(reagan_sents))\n",
    "print('Bush Sentences:', len(bush_sents))\n",
    "print('Clinton Sentences:', len(clinton_sents))\n",
    "print('GWBush Sentences:', len(gwb_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code to randomly select 500 sentences from each presidents corpus\n",
    "# decided to keep just the first 500 so that each pres would be compared \n",
    "# during their same seniority level in office\n",
    "\n",
    "#test = np.asarray(truman_sents)[:,::2].flatten() \n",
    "#test = np.random.choice(test, 500)\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Clinton       2689\n",
       "Truman        2353\n",
       "Eisenhower    2080\n",
       "Johnson       1715\n",
       "Reagan        1680\n",
       "GWBush        1577\n",
       "Bush          1268\n",
       "Nixon          648\n",
       "Kennedy        511\n",
       "Ford           505\n",
       "Carter         415\n",
       "Name: president, dtype: int64"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining sentences and labels into a df and limiting\n",
    "sentences_long = pd.DataFrame(\n",
    "    truman_sents + \n",
    "    ike_sents + \n",
    "    jfk_sents + \n",
    "    lbj_sents + \n",
    "    nixon_sents +\n",
    "    ford_sents +\n",
    "    carter_sents +\n",
    "    reagan_sents +\n",
    "    bush_sents +\n",
    "    clinton_sents +\n",
    "    gwb_sents,\n",
    "    columns=['sentence', 'president']\n",
    ")\n",
    "\n",
    "#confirming class balance dictated by speeker, Carter is low, but wanted to retain\n",
    "#as much info as possible\n",
    "sentences_long.president.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Class Imbalance (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bush          500\n",
       "Eisenhower    500\n",
       "Truman        500\n",
       "GWBush        500\n",
       "Ford          500\n",
       "Kennedy       500\n",
       "Reagan        500\n",
       "Johnson       500\n",
       "Clinton       500\n",
       "Nixon         500\n",
       "Carter        415\n",
       "Name: president, dtype: int64"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining sentences and labels into a df and limiting\n",
    "sentences = pd.DataFrame(\n",
    "    truman_sents[:500] + \n",
    "    ike_sents[:500] + \n",
    "    jfk_sents[:500] + \n",
    "    lbj_sents[:500] + \n",
    "    nixon_sents[:500] +\n",
    "    ford_sents[:500] +\n",
    "    carter_sents +\n",
    "    reagan_sents[:500] +\n",
    "    bush_sents[:500] +\n",
    "    clinton_sents[:500] +\n",
    "    gwb_sents[:500],\n",
    "    columns=['sentence', 'president']\n",
    ")\n",
    "\n",
    "#confirming class balance dictated by speeker, Carter is low, but wanted to retain\n",
    "#as much info as possible\n",
    "sentences.president.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts, POS, & Punc Counts\n",
    "\n",
    "For the sentences I am using the shortened dataset so that the authors are balanced and for computational reasons. Looking for the lemmas in the sentence and then counting them in the sentence is not a fast task given the size of the common words list as well as the length of the sentences in the speeches. Since we are dealing with transcriptions of SOTU addresses, there are some major run-on sentences. Also, the speech writing uses a single or double dash (- or --) to indicate a pause or a break in the speach, which I replaced with a blank space during cleaning. That replacement could have used a period in some cases, but with the size of the raw dataset, it would be difficult to come up with a concrete rule for the re.sub script to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3084"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truman_bow = bag_of_words(truman_doc)\n",
    "ike_bow = bag_of_words(ike_doc)\n",
    "jfk_bow = bag_of_words(jfk_doc)\n",
    "lbj_bow = bag_of_words(lbj_doc)\n",
    "nixon_bow = bag_of_words(nixon_doc)\n",
    "ford_bow = bag_of_words(ford_doc)\n",
    "carter_bow = bag_of_words(carter_doc)\n",
    "reagan_bow = bag_of_words(reagan_doc)\n",
    "bush_bow = bag_of_words(bush_doc)\n",
    "clinton_bow = bag_of_words(clinton_doc)\n",
    "gwb_bow = bag_of_words(gwb_doc)\n",
    "\n",
    "common_words = set(\n",
    "    truman_bow + \n",
    "    ike_bow + \n",
    "    jfk_bow + \n",
    "    lbj_bow + \n",
    "    nixon_bow + \n",
    "    ford_bow + \n",
    "    carter_bow +\n",
    "    reagan_bow +\n",
    "    bush_bow +\n",
    "    clinton_bow +\n",
    "    gwb_bow\n",
    ")\n",
    "\n",
    "len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 10\n",
      "Processing row 20\n",
      "Processing row 30\n",
      "Processing row 40\n",
      "Processing row 50\n",
      "Processing row 60\n",
      "Processing row 70\n",
      "Processing row 80\n",
      "Processing row 90\n",
      "Processing row 100\n",
      "Processing row 110\n",
      "Processing row 120\n",
      "Processing row 130\n",
      "Processing row 140\n",
      "Processing row 150\n",
      "Processing row 160\n",
      "Processing row 170\n",
      "Processing row 180\n",
      "Processing row 190\n",
      "Processing row 200\n",
      "Processing row 210\n",
      "Processing row 220\n",
      "Processing row 230\n",
      "Processing row 240\n",
      "Processing row 250\n",
      "Processing row 260\n",
      "Processing row 270\n",
      "Processing row 280\n",
      "Processing row 290\n",
      "Processing row 300\n",
      "Processing row 310\n",
      "Processing row 320\n",
      "Processing row 330\n",
      "Processing row 340\n",
      "Processing row 350\n",
      "Processing row 360\n",
      "Processing row 370\n",
      "Processing row 380\n",
      "Processing row 390\n",
      "Processing row 400\n",
      "Processing row 410\n",
      "Processing row 420\n",
      "Processing row 430\n",
      "Processing row 440\n",
      "Processing row 450\n",
      "Processing row 460\n",
      "Processing row 470\n",
      "Processing row 480\n",
      "Processing row 490\n",
      "Processing row 500\n",
      "Processing row 510\n",
      "Processing row 520\n",
      "Processing row 530\n",
      "Processing row 540\n",
      "Processing row 550\n",
      "Processing row 560\n",
      "Processing row 570\n",
      "Processing row 580\n",
      "Processing row 590\n",
      "Processing row 600\n",
      "Processing row 610\n",
      "Processing row 620\n",
      "Processing row 630\n",
      "Processing row 640\n",
      "Processing row 650\n",
      "Processing row 660\n",
      "Processing row 670\n",
      "Processing row 680\n",
      "Processing row 690\n",
      "Processing row 700\n",
      "Processing row 710\n",
      "Processing row 720\n",
      "Processing row 730\n",
      "Processing row 740\n",
      "Processing row 750\n",
      "Processing row 760\n",
      "Processing row 770\n",
      "Processing row 780\n",
      "Processing row 790\n",
      "Processing row 800\n",
      "Processing row 810\n",
      "Processing row 820\n",
      "Processing row 830\n",
      "Processing row 840\n",
      "Processing row 850\n",
      "Processing row 860\n",
      "Processing row 870\n",
      "Processing row 880\n",
      "Processing row 890\n",
      "Processing row 900\n",
      "Processing row 910\n",
      "Processing row 920\n",
      "Processing row 930\n",
      "Processing row 940\n",
      "Processing row 950\n",
      "Processing row 960\n",
      "Processing row 970\n",
      "Processing row 980\n",
      "Processing row 990\n",
      "Processing row 1000\n",
      "Processing row 1010\n",
      "Processing row 1020\n",
      "Processing row 1030\n",
      "Processing row 1040\n",
      "Processing row 1050\n",
      "Processing row 1060\n",
      "Processing row 1070\n",
      "Processing row 1080\n",
      "Processing row 1090\n",
      "Processing row 1100\n",
      "Processing row 1110\n",
      "Processing row 1120\n",
      "Processing row 1130\n",
      "Processing row 1140\n",
      "Processing row 1150\n",
      "Processing row 1160\n",
      "Processing row 1170\n",
      "Processing row 1180\n",
      "Processing row 1190\n",
      "Processing row 1200\n",
      "Processing row 1210\n",
      "Processing row 1220\n",
      "Processing row 1230\n",
      "Processing row 1240\n",
      "Processing row 1250\n",
      "Processing row 1260\n",
      "Processing row 1270\n",
      "Processing row 1280\n",
      "Processing row 1290\n",
      "Processing row 1300\n",
      "Processing row 1310\n",
      "Processing row 1320\n",
      "Processing row 1330\n",
      "Processing row 1340\n",
      "Processing row 1350\n",
      "Processing row 1360\n",
      "Processing row 1370\n",
      "Processing row 1380\n",
      "Processing row 1390\n",
      "Processing row 1400\n",
      "Processing row 1410\n",
      "Processing row 1420\n",
      "Processing row 1430\n",
      "Processing row 1440\n",
      "Processing row 1450\n",
      "Processing row 1460\n",
      "Processing row 1470\n",
      "Processing row 1480\n",
      "Processing row 1490\n",
      "Processing row 1500\n",
      "Processing row 1510\n",
      "Processing row 1520\n",
      "Processing row 1530\n",
      "Processing row 1540\n",
      "Processing row 1550\n",
      "Processing row 1560\n",
      "Processing row 1570\n",
      "Processing row 1580\n",
      "Processing row 1590\n",
      "Processing row 1600\n",
      "Processing row 1610\n",
      "Processing row 1620\n",
      "Processing row 1630\n",
      "Processing row 1640\n",
      "Processing row 1650\n",
      "Processing row 1660\n",
      "Processing row 1670\n",
      "Processing row 1680\n",
      "Processing row 1690\n",
      "Processing row 1700\n",
      "Processing row 1710\n",
      "Processing row 1720\n",
      "Processing row 1730\n",
      "Processing row 1740\n",
      "Processing row 1750\n",
      "Processing row 1760\n",
      "Processing row 1770\n",
      "Processing row 1780\n",
      "Processing row 1790\n",
      "Processing row 1800\n",
      "Processing row 1810\n",
      "Processing row 1820\n",
      "Processing row 1830\n",
      "Processing row 1840\n",
      "Processing row 1850\n",
      "Processing row 1860\n",
      "Processing row 1870\n",
      "Processing row 1880\n",
      "Processing row 1890\n",
      "Processing row 1900\n",
      "Processing row 1910\n",
      "Processing row 1920\n",
      "Processing row 1930\n",
      "Processing row 1940\n",
      "Processing row 1950\n",
      "Processing row 1960\n",
      "Processing row 1970\n",
      "Processing row 1980\n",
      "Processing row 1990\n",
      "Processing row 2000\n",
      "Processing row 2010\n",
      "Processing row 2020\n",
      "Processing row 2030\n",
      "Processing row 2040\n",
      "Processing row 2050\n",
      "Processing row 2060\n",
      "Processing row 2070\n",
      "Processing row 2080\n",
      "Processing row 2090\n",
      "Processing row 2100\n",
      "Processing row 2110\n",
      "Processing row 2120\n",
      "Processing row 2130\n",
      "Processing row 2140\n",
      "Processing row 2150\n",
      "Processing row 2160\n",
      "Processing row 2170\n",
      "Processing row 2180\n",
      "Processing row 2190\n",
      "Processing row 2200\n",
      "Processing row 2210\n",
      "Processing row 2220\n",
      "Processing row 2230\n",
      "Processing row 2240\n",
      "Processing row 2250\n",
      "Processing row 2260\n",
      "Processing row 2270\n",
      "Processing row 2280\n",
      "Processing row 2290\n",
      "Processing row 2300\n",
      "Processing row 2310\n",
      "Processing row 2320\n",
      "Processing row 2330\n",
      "Processing row 2340\n",
      "Processing row 2350\n",
      "Processing row 2360\n",
      "Processing row 2370\n",
      "Processing row 2380\n",
      "Processing row 2390\n",
      "Processing row 2400\n",
      "Processing row 2410\n",
      "Processing row 2420\n",
      "Processing row 2430\n",
      "Processing row 2440\n",
      "Processing row 2450\n",
      "Processing row 2460\n",
      "Processing row 2470\n",
      "Processing row 2480\n",
      "Processing row 2490\n",
      "Processing row 2500\n",
      "Processing row 2510\n",
      "Processing row 2520\n",
      "Processing row 2530\n",
      "Processing row 2540\n",
      "Processing row 2550\n",
      "Processing row 2560\n",
      "Processing row 2570\n",
      "Processing row 2580\n",
      "Processing row 2590\n",
      "Processing row 2600\n",
      "Processing row 2610\n",
      "Processing row 2620\n",
      "Processing row 2630\n",
      "Processing row 2640\n",
      "Processing row 2650\n",
      "Processing row 2660\n",
      "Processing row 2670\n",
      "Processing row 2680\n",
      "Processing row 2690\n",
      "Processing row 2700\n",
      "Processing row 2710\n",
      "Processing row 2720\n",
      "Processing row 2730\n",
      "Processing row 2740\n",
      "Processing row 2750\n",
      "Processing row 2760\n",
      "Processing row 2770\n",
      "Processing row 2780\n",
      "Processing row 2790\n",
      "Processing row 2800\n",
      "Processing row 2810\n",
      "Processing row 2820\n",
      "Processing row 2830\n",
      "Processing row 2840\n",
      "Processing row 2850\n",
      "Processing row 2860\n",
      "Processing row 2870\n",
      "Processing row 2880\n",
      "Processing row 2890\n",
      "Processing row 2900\n",
      "Processing row 2910\n",
      "Processing row 2920\n",
      "Processing row 2930\n",
      "Processing row 2940\n",
      "Processing row 2950\n",
      "Processing row 2960\n",
      "Processing row 2970\n",
      "Processing row 2980\n",
      "Processing row 2990\n",
      "Processing row 3000\n",
      "Processing row 3010\n",
      "Processing row 3020\n",
      "Processing row 3030\n",
      "Processing row 3040\n",
      "Processing row 3050\n",
      "Processing row 3060\n",
      "Processing row 3070\n",
      "Processing row 3080\n",
      "Processing row 3090\n",
      "Processing row 3100\n",
      "Processing row 3110\n",
      "Processing row 3120\n",
      "Processing row 3130\n",
      "Processing row 3140\n",
      "Processing row 3150\n",
      "Processing row 3160\n",
      "Processing row 3170\n",
      "Processing row 3180\n",
      "Processing row 3190\n",
      "Processing row 3200\n",
      "Processing row 3210\n",
      "Processing row 3220\n",
      "Processing row 3230\n",
      "Processing row 3240\n",
      "Processing row 3250\n",
      "Processing row 3260\n",
      "Processing row 3270\n",
      "Processing row 3280\n",
      "Processing row 3290\n",
      "Processing row 3300\n",
      "Processing row 3310\n",
      "Processing row 3320\n",
      "Processing row 3330\n",
      "Processing row 3340\n",
      "Processing row 3350\n",
      "Processing row 3360\n",
      "Processing row 3370\n",
      "Processing row 3380\n",
      "Processing row 3390\n",
      "Processing row 3400\n",
      "Processing row 3410\n",
      "Processing row 3420\n",
      "Processing row 3430\n",
      "Processing row 3440\n",
      "Processing row 3450\n",
      "Processing row 3460\n",
      "Processing row 3470\n",
      "Processing row 3480\n",
      "Processing row 3490\n",
      "Processing row 3500\n",
      "Processing row 3510\n",
      "Processing row 3520\n",
      "Processing row 3530\n",
      "Processing row 3540\n",
      "Processing row 3550\n",
      "Processing row 3560\n",
      "Processing row 3570\n",
      "Processing row 3580\n",
      "Processing row 3590\n",
      "Processing row 3600\n",
      "Processing row 3610\n",
      "Processing row 3620\n",
      "Processing row 3630\n",
      "Processing row 3640\n",
      "Processing row 3650\n",
      "Processing row 3660\n",
      "Processing row 3670\n",
      "Processing row 3680\n",
      "Processing row 3690\n",
      "Processing row 3700\n",
      "Processing row 3710\n",
      "Processing row 3720\n",
      "Processing row 3730\n",
      "Processing row 3740\n",
      "Processing row 3750\n",
      "Processing row 3760\n",
      "Processing row 3770\n",
      "Processing row 3780\n",
      "Processing row 3790\n",
      "Processing row 3800\n",
      "Processing row 3810\n",
      "Processing row 3820\n",
      "Processing row 3830\n",
      "Processing row 3840\n",
      "Processing row 3850\n",
      "Processing row 3860\n",
      "Processing row 3870\n",
      "Processing row 3880\n",
      "Processing row 3890\n",
      "Processing row 3900\n",
      "Processing row 3910\n",
      "Processing row 3920\n",
      "Processing row 3930\n",
      "Processing row 3940\n",
      "Processing row 3950\n",
      "Processing row 3960\n",
      "Processing row 3970\n",
      "Processing row 3980\n",
      "Processing row 3990\n",
      "Processing row 4000\n",
      "Processing row 4010\n",
      "Processing row 4020\n",
      "Processing row 4030\n",
      "Processing row 4040\n",
      "Processing row 4050\n",
      "Processing row 4060\n",
      "Processing row 4070\n",
      "Processing row 4080\n",
      "Processing row 4090\n",
      "Processing row 4100\n",
      "Processing row 4110\n",
      "Processing row 4120\n",
      "Processing row 4130\n",
      "Processing row 4140\n",
      "Processing row 4150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 4160\n",
      "Processing row 4170\n",
      "Processing row 4180\n",
      "Processing row 4190\n",
      "Processing row 4200\n",
      "Processing row 4210\n",
      "Processing row 4220\n",
      "Processing row 4230\n",
      "Processing row 4240\n",
      "Processing row 4250\n",
      "Processing row 4260\n",
      "Processing row 4270\n",
      "Processing row 4280\n",
      "Processing row 4290\n",
      "Processing row 4300\n",
      "Processing row 4310\n",
      "Processing row 4320\n",
      "Processing row 4330\n",
      "Processing row 4340\n",
      "Processing row 4350\n",
      "Processing row 4360\n",
      "Processing row 4370\n",
      "Processing row 4380\n",
      "Processing row 4390\n",
      "Processing row 4400\n",
      "Processing row 4410\n",
      "Processing row 4420\n",
      "Processing row 4430\n",
      "Processing row 4440\n",
      "Processing row 4450\n",
      "Processing row 4460\n",
      "Processing row 4470\n",
      "Processing row 4480\n",
      "Processing row 4490\n",
      "Processing row 4500\n",
      "Processing row 4510\n",
      "Processing row 4520\n",
      "Processing row 4530\n",
      "Processing row 4540\n",
      "Processing row 4550\n",
      "Processing row 4560\n",
      "Processing row 4570\n",
      "Processing row 4580\n",
      "Processing row 4590\n",
      "Processing row 4600\n",
      "Processing row 4610\n",
      "Processing row 4620\n",
      "Processing row 4630\n",
      "Processing row 4640\n",
      "Processing row 4650\n",
      "Processing row 4660\n",
      "Processing row 4670\n",
      "Processing row 4680\n",
      "Processing row 4690\n",
      "Processing row 4700\n",
      "Processing row 4710\n",
      "Processing row 4720\n",
      "Processing row 4730\n",
      "Processing row 4740\n",
      "Processing row 4750\n",
      "Processing row 4760\n",
      "Processing row 4770\n",
      "Processing row 4780\n",
      "Processing row 4790\n",
      "Processing row 4800\n",
      "Processing row 4810\n",
      "Processing row 4820\n",
      "Processing row 4830\n",
      "Processing row 4840\n",
      "Processing row 4850\n",
      "Processing row 4860\n",
      "Processing row 4870\n",
      "Processing row 4880\n",
      "Processing row 4890\n",
      "Processing row 4900\n",
      "Processing row 4910\n",
      "Processing row 4920\n",
      "Processing row 4930\n",
      "Processing row 4940\n",
      "Processing row 4950\n",
      "Processing row 4960\n",
      "Processing row 4970\n",
      "Processing row 4980\n",
      "Processing row 4990\n",
      "Processing row 5000\n",
      "Processing row 5010\n",
      "Processing row 5020\n",
      "Processing row 5030\n",
      "Processing row 5040\n",
      "Processing row 5050\n",
      "Processing row 5060\n",
      "Processing row 5070\n",
      "Processing row 5080\n",
      "Processing row 5090\n",
      "Processing row 5100\n",
      "Processing row 5110\n",
      "Processing row 5120\n",
      "Processing row 5130\n",
      "Processing row 5140\n",
      "Processing row 5150\n",
      "Processing row 5160\n",
      "Processing row 5170\n",
      "Processing row 5180\n",
      "Processing row 5190\n",
      "Processing row 5200\n",
      "Processing row 5210\n",
      "Processing row 5220\n",
      "Processing row 5230\n",
      "Processing row 5240\n",
      "Processing row 5250\n",
      "Processing row 5260\n",
      "Processing row 5270\n",
      "Processing row 5280\n",
      "Processing row 5290\n",
      "Processing row 5300\n",
      "Processing row 5310\n",
      "Processing row 5320\n",
      "Processing row 5330\n",
      "Processing row 5340\n",
      "Processing row 5350\n",
      "Processing row 5360\n",
      "Processing row 5370\n",
      "Processing row 5380\n",
      "Processing row 5390\n",
      "Processing row 5400\n",
      "Processing row 5410\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-pron-</th>\n",
       "      <th>passage</th>\n",
       "      <th>favor</th>\n",
       "      <th>prosper</th>\n",
       "      <th>assault</th>\n",
       "      <th>analysis</th>\n",
       "      <th>ease</th>\n",
       "      <th>individual</th>\n",
       "      <th>worthy</th>\n",
       "      <th>prior</th>\n",
       "      <th>...</th>\n",
       "      <th>neutrality</th>\n",
       "      <th>issue</th>\n",
       "      <th>servant</th>\n",
       "      <th>unhindered</th>\n",
       "      <th>executive</th>\n",
       "      <th>operation</th>\n",
       "      <th>saddle</th>\n",
       "      <th>scholarship</th>\n",
       "      <th>speech_sentence</th>\n",
       "      <th>speaking_president</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(A, April, 16, ,, 1945, Mr., Speaker, ,, Mr., ...</td>\n",
       "      <td>Truman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Only, yesterday, ,, we, laid, to, rest, the, ...</td>\n",
       "      <td>Truman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(At, a, time, like, this, ,, words, are, inade...</td>\n",
       "      <td>Truman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, most, eloquent, tribute, would, be, a, r...</td>\n",
       "      <td>Truman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Yet, ,, in, this, decisive, hour, ,, when, wo...</td>\n",
       "      <td>Truman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3086 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  -pron- passage favor prosper assault analysis ease individual worthy prior  \\\n",
       "0      0       0     0       0       0        0    0          0      0     0   \n",
       "1      0       0     0       0       0        0    0          0      0     0   \n",
       "2      0       0     0       0       0        0    0          0      0     0   \n",
       "3      0       0     0       0       0        0    0          0      0     0   \n",
       "4      0       0     0       0       0        0    0          0      0     0   \n",
       "\n",
       "         ...         neutrality issue servant unhindered executive operation  \\\n",
       "0        ...                  0     0       0          0         0         0   \n",
       "1        ...                  0     0       0          0         0         0   \n",
       "2        ...                  0     0       0          0         0         0   \n",
       "3        ...                  0     0       0          0         0         0   \n",
       "4        ...                  0     0       0          0         0         0   \n",
       "\n",
       "  saddle scholarship                                    speech_sentence  \\\n",
       "0      0           0  (A, April, 16, ,, 1945, Mr., Speaker, ,, Mr., ...   \n",
       "1      0           0  (Only, yesterday, ,, we, laid, to, rest, the, ...   \n",
       "2      0           0  (At, a, time, like, this, ,, words, are, inade...   \n",
       "3      0           0  (The, most, eloquent, tribute, would, be, a, r...   \n",
       "4      0           0  (Yet, ,, in, this, decisive, hour, ,, when, wo...   \n",
       "\n",
       "  speaking_president  \n",
       "0             Truman  \n",
       "1             Truman  \n",
       "2             Truman  \n",
       "3             Truman  \n",
       "4             Truman  \n",
       "\n",
       "[5 rows x 3086 columns]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = bow_features(sentences, common_words)\n",
    "word_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "truman_pos = bag_of_pos(truman_doc)\n",
    "ike_pos = bag_of_pos(ike_doc)\n",
    "jfk_pos = bag_of_pos(jfk_doc)\n",
    "lbj_pos = bag_of_pos(lbj_doc)\n",
    "nixon_pos = bag_of_pos(nixon_doc)\n",
    "ford_pos = bag_of_pos(ford_doc)\n",
    "carter_pos = bag_of_pos(carter_doc)\n",
    "reagan_pos = bag_of_pos(reagan_doc)\n",
    "bush_pos = bag_of_pos(bush_doc)\n",
    "clinton_pos = bag_of_pos(clinton_doc)\n",
    "gwb_pos = bag_of_pos(gwb_doc)\n",
    "\n",
    "common_pos = set(\n",
    "    truman_pos +\n",
    "    ike_pos +\n",
    "    jfk_pos +\n",
    "    lbj_pos +\n",
    "    nixon_pos +\n",
    "    ford_pos +\n",
    "    carter_pos +\n",
    "    reagan_pos +\n",
    "    bush_pos +\n",
    "    clinton_pos +\n",
    "    gwb_pos\n",
    ")\n",
    "\n",
    "print(len(common_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n",
      "Processing row 450\n",
      "Processing row 500\n",
      "Processing row 550\n",
      "Processing row 600\n",
      "Processing row 650\n",
      "Processing row 700\n",
      "Processing row 750\n",
      "Processing row 800\n",
      "Processing row 850\n",
      "Processing row 900\n",
      "Processing row 950\n",
      "Processing row 1000\n",
      "Processing row 1050\n",
      "Processing row 1100\n",
      "Processing row 1150\n",
      "Processing row 1200\n",
      "Processing row 1250\n",
      "Processing row 1300\n",
      "Processing row 1350\n",
      "Processing row 1400\n",
      "Processing row 1450\n",
      "Processing row 1500\n",
      "Processing row 1550\n",
      "Processing row 1600\n",
      "Processing row 1650\n",
      "Processing row 1700\n",
      "Processing row 1750\n",
      "Processing row 1800\n",
      "Processing row 1850\n",
      "Processing row 1900\n",
      "Processing row 1950\n",
      "Processing row 2000\n",
      "Processing row 2050\n",
      "Processing row 2100\n",
      "Processing row 2150\n",
      "Processing row 2200\n",
      "Processing row 2250\n",
      "Processing row 2300\n",
      "Processing row 2350\n",
      "Processing row 2400\n",
      "Processing row 2450\n",
      "Processing row 2500\n",
      "Processing row 2550\n",
      "Processing row 2600\n",
      "Processing row 2650\n",
      "Processing row 2700\n",
      "Processing row 2750\n",
      "Processing row 2800\n",
      "Processing row 2850\n",
      "Processing row 2900\n",
      "Processing row 2950\n",
      "Processing row 3000\n",
      "Processing row 3050\n",
      "Processing row 3100\n",
      "Processing row 3150\n",
      "Processing row 3200\n",
      "Processing row 3250\n",
      "Processing row 3300\n",
      "Processing row 3350\n",
      "Processing row 3400\n",
      "Processing row 3450\n",
      "Processing row 3500\n",
      "Processing row 3550\n",
      "Processing row 3600\n",
      "Processing row 3650\n",
      "Processing row 3700\n",
      "Processing row 3750\n",
      "Processing row 3800\n",
      "Processing row 3850\n",
      "Processing row 3900\n",
      "Processing row 3950\n",
      "Processing row 4000\n",
      "Processing row 4050\n",
      "Processing row 4100\n",
      "Processing row 4150\n",
      "Processing row 4200\n",
      "Processing row 4250\n",
      "Processing row 4300\n",
      "Processing row 4350\n",
      "Processing row 4400\n",
      "Processing row 4450\n",
      "Processing row 4500\n",
      "Processing row 4550\n",
      "Processing row 4600\n",
      "Processing row 4650\n",
      "Processing row 4700\n",
      "Processing row 4750\n",
      "Processing row 4800\n",
      "Processing row 4850\n",
      "Processing row 4900\n",
      "Processing row 4950\n",
      "Processing row 5000\n",
      "Processing row 5050\n",
      "Processing row 5100\n",
      "Processing row 5150\n",
      "Processing row 5200\n",
      "Processing row 5250\n",
      "Processing row 5300\n",
      "Processing row 5350\n",
      "Processing row 5400\n"
     ]
    }
   ],
   "source": [
    "pos = pos_features(sentences['sentence'], common_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.concat([word_count, pos], axis=1)\n",
    "master_df.drop('text_sentence', inplace=True, axis=1)\n",
    "master_df = master_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df['punc_count'] = pd.Series(amount_punc(i) for i in master_df['speech_sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Prediction of President"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4061\n",
      "1354\n"
     ]
    }
   ],
   "source": [
    "X = master_df.drop(['speech_sentence', 'speaking_president', 'pol_party'], axis=1)\n",
    "Y = master_df.loc[:, 'speaking_president']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=86)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  8.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 17.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   19.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   59.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.6048282012084276, 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "# going with logistic regression again so that we can compare\n",
    "# unsupervised component scores to normal features\n",
    "\n",
    "#hard coding multinomial and penalty due to solver restrictions for multinomial models\n",
    "lr = LogisticRegression(penalty='l2', multi_class='multinomial', tol=1e-3)\n",
    "\n",
    "params = {\n",
    "    'C':Real(0.001, 1, 'uniform'),\n",
    "    'solver':Categorical(['newton-cg', 'sag', 'saga', 'lbfgs'])\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    lr,\n",
    "    params,\n",
    "    cv=5,\n",
    "    n_iter=5,\n",
    "    random_state=645,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "opt.fit(X_train, Y_train)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________Training Statistics__________\n",
      "\n",
      "[[311   4  16   4   5   6   4   3   5   6   4]\n",
      " [ 14 245  14   7   7   7   5   3   9   4   7]\n",
      " [ 27   4 300   0  10   5   1   2   6   4   5]\n",
      " [ 11   7   6 326   5   2  11   0   3   5  12]\n",
      " [ 18   3  12   4 306   5   6   2   9   6   6]\n",
      " [ 31   3   4   3   5 304   6   0   2   4   5]\n",
      " [ 22   3  10   6   7  10 293   5  10   6   6]\n",
      " [ 27   3   4   6   2   4   7 316   3   3   4]\n",
      " [ 13   6   7   2   8   6   3   5 317   5   9]\n",
      " [ 22   6  18   6   5   6   5   4   7 278   3]\n",
      " [ 15   6   4  11   5   3   5   4   6   1 317]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bush       0.61      0.85      0.71       368\n",
      "      Carter       0.84      0.76      0.80       322\n",
      "     Clinton       0.76      0.82      0.79       364\n",
      "  Eisenhower       0.87      0.84      0.85       388\n",
      "        Ford       0.84      0.81      0.82       377\n",
      "      GWBush       0.85      0.83      0.84       367\n",
      "     Johnson       0.85      0.78      0.81       378\n",
      "     Kennedy       0.92      0.83      0.87       379\n",
      "       Nixon       0.84      0.83      0.84       381\n",
      "      Reagan       0.86      0.77      0.82       360\n",
      "      Truman       0.84      0.84      0.84       377\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      4061\n",
      "   macro avg       0.83      0.81      0.82      4061\n",
      "weighted avg       0.83      0.82      0.82      4061\n",
      "\n",
      "\n",
      "__________Test Statistics__________\n",
      "\n",
      "[[48  5 13  9  7 12  7  5 10  9  7]\n",
      " [10 18  8 13  9  4  2  7 14  3  5]\n",
      " [19  7 57  0  7  9 11  9  6 10  1]\n",
      " [ 7  2  1 48  3  1  7  8  9  5 21]\n",
      " [18  7 16  7 33  8 10  7  4  9  4]\n",
      " [21  5  7  1  8 61  7  5  5  4  9]\n",
      " [14  5  7  8  8  5 35 11 11 10  8]\n",
      " [ 8  5  6 10  6  3 11 47 12  4  9]\n",
      " [11  7  8  9  3  5  9  5 51  6  5]\n",
      " [18  4 16 11 15  5  7  6 11 40  7]\n",
      " [ 8  6  3 12  7  2  7  8  6  4 60]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bush       0.26      0.36      0.31       132\n",
      "      Carter       0.25      0.19      0.22        93\n",
      "     Clinton       0.40      0.42      0.41       136\n",
      "  Eisenhower       0.38      0.43      0.40       112\n",
      "        Ford       0.31      0.27      0.29       123\n",
      "      GWBush       0.53      0.46      0.49       133\n",
      "     Johnson       0.31      0.29      0.30       122\n",
      "     Kennedy       0.40      0.39      0.39       121\n",
      "       Nixon       0.37      0.43      0.40       119\n",
      "      Reagan       0.38      0.29      0.33       140\n",
      "      Truman       0.44      0.49      0.46       123\n",
      "\n",
      "   micro avg       0.37      0.37      0.37      1354\n",
      "   macro avg       0.37      0.36      0.36      1354\n",
      "weighted avg       0.37      0.37      0.37      1354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    multi_class='multinomial',\n",
    "    C=0.604,\n",
    "    solver='newton-cg'\n",
    ")\n",
    "\n",
    "lr.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_pred_mcl = lr.predict(X_train)\n",
    "Y_test_pred_mcl = lr.predict(X_test)\n",
    "\n",
    "print('__________Training Statistics__________\\n')\n",
    "print(confusion_matrix(Y_train, Y_train_pred_mcl))\n",
    "print(classification_report(Y_train, Y_train_pred_mcl))\n",
    "\n",
    "print('\\n__________Test Statistics__________\\n')\n",
    "print(confusion_matrix(Y_test, Y_test_pred_mcl))\n",
    "print(classification_report(Y_test, Y_test_pred_mcl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfit, overfit, overfit. There is a problem with class imbalance and there is also a problem with how that was handled. By diminishing the number of rows to just the first 500, there is a chance that there are words in the training set that are never used in the test set, or vice versa. If the test said includes things not seen in training then it will not be able to predict the speaker very well. I think that is the issue with this dataset. More lines and a more sophisticated way of handling class imbalance are key here.\n",
    "\n",
    "With that in mind, the raw dataset includes 1.5 million words that were distilled down to the most common ~3000. That means I am only using 0.2% if the potential data to build these models (that 1.5 mil does include stop words), which is just to say there is a tremendous amount of potential here. \n",
    "\n",
    "However, when comparing these results to the unsupervised approach earlier, our training scores were much higher. Yes, the overfit does hurt, but I think there is more promise with this dataset when going the more conventional feature generation route. \n",
    "\n",
    "Let's see if we can do better predicting party affiliations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classifier of Political Party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REP    3000\n",
       "DEM    2415\n",
       "Name: pol_party, dtype: int64"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assigning political party affiliations to each president\n",
    "party = {\n",
    "    'Truman':'DEM', \n",
    "    'Eisenhower':'REP', \n",
    "    'Kennedy':'DEM', \n",
    "    'Johnson':'DEM', \n",
    "    'Nixon':'REP', \n",
    "    'Ford':'REP', \n",
    "    'Carter':'DEM', \n",
    "    'Reagan':'REP', \n",
    "    'Bush':'REP', \n",
    "    'Clinton':'DEM', \n",
    "    'GWBush':'REP'\n",
    "}\n",
    "for pres in party:\n",
    "    master_df.loc[master_df['speaking_president'] == pres, 'pol_party'] = party[pres]\n",
    "\n",
    "#checking for class balance    \n",
    "master_df['pol_party'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4061\n",
      "1354\n"
     ]
    }
   ],
   "source": [
    "X = master_df.drop(['speech_sentence', 'speaking_president', 'pol_party'], axis=1)\n",
    "Y = master_df.loc[:, 'pol_party']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=86)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    8.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    6.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    6.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    6.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.7219406141637149, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "params = {\n",
    "    'penalty':Categorical(['l1', 'l2']),\n",
    "    'C':Real(0.01, 1, 'uniform'),\n",
    "    'solver':Categorical(['saga', 'liblinear']), #only these solvers can handle both penalties\n",
    "    \n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    lr,\n",
    "    params,\n",
    "    cv=5,\n",
    "    n_iter=7,\n",
    "    random_state=645,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "opt.fit(X_train, Y_train)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________Training Statistics__________\n",
      "\n",
      "[[1377  443]\n",
      " [ 218 2023]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DEM       0.86      0.76      0.81      1820\n",
      "         REP       0.82      0.90      0.86      2241\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      4061\n",
      "   macro avg       0.84      0.83      0.83      4061\n",
      "weighted avg       0.84      0.84      0.84      4061\n",
      "\n",
      "\n",
      "__________Test Statistics__________\n",
      "\n",
      "[[326 269]\n",
      " [202 557]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DEM       0.62      0.55      0.58       595\n",
      "         REP       0.67      0.73      0.70       759\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      1354\n",
      "   macro avg       0.65      0.64      0.64      1354\n",
      "weighted avg       0.65      0.65      0.65      1354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=0.7219,\n",
    "    solver='liblinear'\n",
    ")\n",
    "\n",
    "lr.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_pred = lr.predict(X_train)\n",
    "Y_test_pred = lr.predict(X_test)\n",
    "\n",
    "print('__________Training Statistics__________\\n')\n",
    "print(confusion_matrix(Y_train, Y_train_pred))\n",
    "print(classification_report(Y_train, Y_train_pred))\n",
    "\n",
    "print('\\n__________Test Statistics__________\\n')\n",
    "print(confusion_matrix(Y_test, Y_test_pred))\n",
    "print(classification_report(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfit isn't as bad for the party predictions and precision and recall are both looking good. There is something to say in regards to party politics over the last 60 years, these presidents really stuck to their talking points! It is apparent that the most common words used in the SOTU addresses can be traced back to the two political parties even when using a subset of the data. Also, considering that the common_word list only retained words that were present in all of the speeches and I limited them to only the top words used, means that party divisions and political affiliations are at the forefront of these addresses.\n",
    "\n",
    "It's very telling that the party prediction was so effective for each of the sentences. Given the inclusion of the whole dataset and a much larger set of words, the overfit issue shown here will likely be less of a problem. There is a 160 sentence swing between the DEM sentences and the Rep sentences in the test set. When the test set is only 1300 sentences long, that means the imbalance is >10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion's of Unsupervised vs. Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though there is a lot of data from their speeches missing in comparison to the unsupervised models, the performance is still better for the normal supervised approaches. A future iteration of this project should be to build a word2vec model and use that to look for similarity between current day politicians/presidents and compare them back to the past. It would be interesting to see if there are any political similarities outside of the model once a connection is made. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
